# ============================================================================
# BoeNet v2.0.1 Phase 2: K=2 Depth Sweep (Lean Version)
# ============================================================================
#
# PURPOSE: Test if DEPTH is the key variable for K=2 performance
#          Quick 6-run sweep to validate hypothesis before full exploration
#          Prepare best model for USER INTERACTION testing
#
# ============================================================================
# THE PIVOT: DEPTH > K
# ============================================================================
#
# WHAT WE LEARNED FROM PHASE 1:
# -----------------------------
# All tree models (K=2,3,4) plateau around 11.63-11.70 PPL at depth=2:
#
#   | K | Depth | Max Nodes | Best PPL | Time/Epoch |
#   |---|-------|-----------|----------|------------|
#   | 0 | N/A   | 1         | 11.55    | 13 sec     |
#   | 2 | 2     | 7         | 11.64    | 95 sec     |
#   | 3 | 2     | 13        | 11.63    | 320 sec    |
#   | 4 | 2     | 21        | 11.70    | 330 sec    |
#
# KEY INSIGHT: The bottleneck is NOT K (branching factor),
#              but DEPTH (reasoning levels)!
#
# NEW HYPOTHESIS:
# ---------------
# K=2 + More Depth = Better than K=3/K=4 at depth=2
#
#   | Depth | K=2 Nodes | Reasoning Levels | Expected PPL |
#   |-------|-----------|------------------|--------------|
#   | 2     | 7         | 3 levels         | 11.64 (known)|
#   | 3     | 15        | 4 levels         | ~11.58?      |
#   | 4     | 31        | 5 levels         | ~11.54?      |
#
# If depth=4 matches K=0 (11.55), we've found the optimal architecture!
#
# ============================================================================
# BINARY TREE DEPTH VISUALIZATION
# ============================================================================
#
# DEPTH=2 (Current - 7 nodes, 3 reasoning levels):
#
#         Root (Level 0: Initial thought)
#         /  \
#        A    B (Level 1: Yes/No)
#       /\    /\
#      1  2  3  4 (Level 2: Confidence)
#
# DEPTH=3 (15 nodes, 4 reasoning levels):
#
#              Root (Level 0: Initial thought)
#             /    \
#            A      B (Level 1: Yes/No)
#           /\      /\
#          1  2    3  4 (Level 2: Confidence)
#         /\ /\   /\ /\
#        8 leaf nodes (Level 3: Evidence)
#
# DEPTH=4 (31 nodes, 5 reasoning levels):
#
#                   Root (Level 0: Initial thought)
#                  /    \
#                 A      B (Level 1: Yes/No)
#                /\      /\
#               ...    ... (Level 2: Confidence)
#              /\  /\  /\  /\
#             ... ... ... ... (Level 3: Evidence)
#            /\ /\ /\ /\ /\ /\ /\ /\
#           16 leaf nodes (Level 4: Context/Refinement)
#
# REASONING INTERPRETATION:
# -------------------------
# Depth=2: "Is this likely? → Yes/No → How confident?"
# Depth=3: "Is this likely? → Yes/No → Confident? → What evidence?"
# Depth=4: "Is this likely? → Yes/No → Confident? → Evidence? → Context?"
#
# More depth = More reasoning steps = Better predictions
#
# ============================================================================
# THIS SWEEP: 6 RUNS (~6 hours)
# ============================================================================
#
# Configuration:
#   K=2 only (binary reasoning - proven winner)
#   Depths: 2, 3, 4 (the key variable)
#   λ=0.05 (best from Phase 1)
#   threshold=0.50 (best sparsity)
#   Epochs: 20, 30 (convergence check)
#
# Run count: 3 depths × 1 λ × 1 threshold × 2 epochs = 6 runs
#
# Time estimate:
#   Depth=2: 2 runs × ~35 min = ~70 min
#   Depth=3: 2 runs × ~56 min = ~112 min
#   Depth=4: 2 runs × ~94 min = ~188 min
#   Total: ~6 hours
#
# ============================================================================
# WHAT HAPPENS AFTER THIS SWEEP
# ============================================================================
#
# STEP 1: Analyze results
#   - Does depth=3 or depth=4 beat depth=2?
#   - Does deeper K=2 match K=0 baseline (11.55)?
#
# STEP 2: User interaction testing (YOU test the best model)
#   docker run -it --gpus all \
#     -v ${PWD}/runs:/app/runs \
#     boenet:cuda python infer_boenet.py \
#       --ckpt runs/BEST_MODEL/model.pt \
#       --generate --max_tokens 200 --temperature 0.8
#
# STEP 3: If results are promising → Develop DYNAMIC DEPTH algorithm
#   Instead of fixed max_depth, the model decides:
#   - "Do I need to think deeper about this?"
#   - Like true BFS: expand until solution found
#   - Adaptive reasoning depth per input
#
# ============================================================================
# FUTURE: DYNAMIC DEPTH ALGORITHM (After This Sweep)
# ============================================================================
#
# CURRENT APPROACH (Fixed Depth):
#   max_depth = 4  # Hardcoded
#   # Every input gets same depth, regardless of difficulty
#
# PROPOSED APPROACH (Dynamic Depth):
#   # Model decides when to stop expanding
#   while not confident_enough and depth < max_depth:
#       expand_next_level()
#       depth += 1
#
# HOW IT WOULD WORK:
#   1. Start with root node
#   2. Policy decides: "Should I expand deeper?"
#   3. If yes → expand children, repeat
#   4. If no → stop, make prediction
#   5. Different inputs get different depths!
#
# BENEFITS:
#   - Easy inputs: depth=1-2 (fast)
#   - Hard inputs: depth=4-5 (accurate)
#   - True adaptive computation
#   - Like human System 1 vs System 2 thinking
#
# IMPLEMENTATION IDEAS:
#   - Add "stop" action to policy network
#   - Train with reward that penalizes unnecessary depth
#   - Or: confidence threshold - stop when prediction is confident
#
# This is the REAL innovation: not just K or fixed depth,
# but LEARNED ADAPTIVE DEPTH based on input complexity!
#
# ============================================================================

sweep:
  # ==========================================================================
  # LANGUAGE MODEL PARAMETERS (FIXED - PROVEN VALUES)
  # ==========================================================================
  
  # Vocabulary size: FIXED at 256 (character-level ByteTokenizer)
  vocab_size: 256
  
  # Sequence length: FIXED at 128 (good balance of context and speed)
  seq_len_list: [128]
  
  # Embedding dimension: FIXED at 64 (proven to work well)
  embed_dim_list: [64]
  
  # Dataset: WikiText-2 (consistent with all previous runs)
  dataset: wikitext2
  
  # ==========================================================================
  # K=2 ONLY - BINARY REASONING TREE (PROVEN WINNER)
  # ==========================================================================
  #
  # K=2 won Phase 1 decisively:
  #   - Same PPL as K=3 (11.64 vs 11.63)
  #   - 3.4x faster training than K=3
  #   - 2x faster inference than K=3
  #   - Handles sparsity gracefully (79% sparse, PPL improves!)
  #   - GPU-efficient (power of 2)
  #
  # K=4 was eliminated:
  #   - Worse PPL (11.70)
  #   - Sparsity HURTS quality (PPL jumps to 11.91)
  #   - Slower than K=2
  #
  k_values: [0,2]
  
  # ==========================================================================
  # DEPTH SWEEP - THE KEY VARIABLE (MAIN EXPERIMENT)
  # ==========================================================================
  #
  # THIS IS WHAT WE'RE TESTING
  #
  # All previous runs used depth=2. Now testing if more depth helps.
  #
  # | Depth | K=2 Nodes | Reasoning Levels | vs K=0 (11.55) |
  # |-------|-----------|------------------|----------------|
  # | 2     | 7         | 3 levels         | +0.09 worse    |
  # | 3     | 15        | 4 levels         | ??? (testing)  |
  # | 4     | 31        | 5 levels         | ??? (testing)  |
  #
  # HYPOTHESIS: Deeper trees will close the gap with K=0
  #
  max_depths: [2, 3, 4]
  
  # ==========================================================================
  # POLICY PARAMETERS - BEST FROM PHASE 1
  # ==========================================================================
  
  # Lambda efficiency: FIXED at 0.05 (best from Phase 1)
  # This gave 79% sparsity with best PPL
  lambda_efficiency_list: [0.05]
  
  # Greedy threshold: FIXED at 0.50 (enables sparsity)
  # This is where K=2 shines - sparse but accurate
  greedy_threshold_list: [0.50]
  
  # Number of rollouts: FIXED at 3
  num_rollouts_list: [3]
  
  # Entropy bonus: FIXED at 0.01
  beta_entropy_list: [0.01]
  
  # Policy loss weight: FIXED at 0.5
  beta_policy_list: [0.5]
  
  # ==========================================================================
  # ARCHITECTURE PARAMETERS - FIXED
  # ==========================================================================
  
  # Hidden dimensions: FIXED at 128
  hidden_dims: [128]
  
  # Pooling: FIXED at mean
  poolings: [mean]
  
  # ==========================================================================
  # OPTIMIZATION - FIXED (PROVEN VALUES)
  # ==========================================================================
  
  # Learning rate: FIXED at 0.001
  lrs: [0.001]
  
  # Batch size: FIXED at 64
  batch_sizes: [64]
  
  # Weight decay: FIXED at 0.0
  weight_decays: [0.0]
  
  # ==========================================================================
  # EPOCHS - CONVERGENCE CHECK
  # ==========================================================================
  #
  # Two key values to check convergence:
  #   20: Extended training
  #   30: Full training (ensure convergence)
  #
  # We skip 10, 15 since we have that data from Phase 1 for depth=2
  # Deeper trees may need more epochs to converge
  #
  epochs_list: [10,20, 30]

# ============================================================================
# TRAINING SETTINGS
# ============================================================================
training:
  # Default epochs (overridden by epochs_list)
  epochs: 20
  
  # Single run per config
  repeats: 1
  
  # Random seed (consistent with all previous runs)
  seed0: 42
  
  # Dataset
  dataset: wikitext2

# ============================================================================
# INFERENCE SETTINGS
# ============================================================================
inference:
  # Number of samples for perplexity measurement
  infer_samples: 1000
  
  # Force CPU inference for fair comparison
  cpu_only: true
  
  # Enable policy analysis
  debug_policy: true

# ============================================================================
# PATHS
# ============================================================================
paths:
  # Output directory
  save_root: runs
  
  # Dataset directory
  data_root: ./data
  
  # Training script (v2.0.1 with stability fixes)
  train_script: train_boenet.py
  
  # Inference script
  infer_script: infer_boenet.py

# ============================================================================
# RUN COUNT CALCULATION
# ============================================================================
#
# K=2 depth sweep (lean version):
#   3 depths × 1 λ × 1 threshold × 2 epochs = 6 runs
#
# Time estimate per depth:
#   Depth=2 (7 nodes):  ~95 sec/epoch
#   Depth=3 (15 nodes): ~150 sec/epoch (estimated)
#   Depth=4 (31 nodes): ~250 sec/epoch (estimated)
#
# Per run (average 25 epochs):
#   Depth=2: 25 × 95 sec = ~40 min/run × 2 runs = ~80 min
#   Depth=3: 25 × 150 sec = ~63 min/run × 2 runs = ~126 min
#   Depth=4: 25 × 250 sec = ~104 min/run × 2 runs = ~208 min
#
# Total: ~414 min = ~7 hours
#
# Note: Depth=2 runs will complete first (~80 min),
#       giving quick comparison data while depth=3,4 continue.
#
# ============================================================================

# ============================================================================
# EXPECTED RESULTS
# ============================================================================
#
# BASELINE (from Phase 1):
#   K=0: PPL = 11.55 (target to match)
#   K=2 depth=2: PPL = 11.64
#
# PREDICTIONS:
#   | Depth | Expected PPL | vs K=0 | Reasoning |
#   |-------|--------------|--------|-----------|
#   | 2     | 11.64        | +0.09  | Known     |
#   | 3     | ~11.58       | +0.03  | 1 more reasoning level |
#   | 4     | ~11.54       | -0.01  | 2 more reasoning levels |
#
# IF DEPTH=4 MATCHES OR BEATS K=0:
#   → Binary reasoning tree is viable architecture
#   → Depth is the key, not K
#   → Proceed to user interaction testing
#   → Then develop dynamic depth algorithm
#
# IF DEPTH=4 STILL WORSE THAN K=0:
#   → Dense (K=0) fundamentally better for this task
#   → Tree structure adds overhead without benefit
#   → May need larger model or different approach
#
# ============================================================================

# ============================================================================
# USAGE INSTRUCTIONS
# ============================================================================
#
# 1. STOP CURRENT SWEEP:
#    docker rm -f boenet_phase1
#
# 2. SAVE THIS CONFIG:
#    Save as: configs/experiment-config.yaml
#
# 3. START DEPTH SWEEP:
#    docker run -d --gpus all --name boenet_depth_test \
#      -v ${PWD}/data:/app/data \
#      -v ${PWD}/runs:/app/runs \
#      -v ${PWD}/configs:/app/configs \
#      -v ${PWD}/boenet:/app/boenet \
#      boenet:cuda python boenet_training_matrix.py \
#        --config configs/experiment-config.yaml
#
# 4. MONITOR PROGRESS:
#    docker logs -f boenet_depth_test
#
# 5. CHECK GPU UTILIZATION:
#    nvidia-smi -l 5
#
# 6. VIEW RESULTS AS THEY COMPLETE:
#    # Depth=2 runs complete first (~80 min)
#    docker exec boenet_depth_test cat runs/*/matrix_results.csv
#
# ============================================================================

# ============================================================================
# AFTER SWEEP: USER INTERACTION TESTING
# ============================================================================
#
# Once the sweep completes and we identify the best depth:
#
# 1. FIND THE BEST MODEL:
#    Look at matrix_results.csv for lowest infer_val_ppl
#    Note the checkpoint_path for that run
#
# 2. GENERATE TEXT (Interactive):
#    docker run -it --gpus all \
#      -v ${PWD}/runs:/app/runs \
#      boenet:cuda python infer_boenet.py \
#        --ckpt runs/TIMESTAMP/BEST_RUN/model.pt \
#        --generate \
#        --max_tokens 200 \
#        --temperature 0.8
#
# 3. TEST WITH PROMPTS:
#    docker run -it --gpus all \
#      -v ${PWD}/runs:/app/runs \
#      boenet:cuda python infer_boenet.py \
#        --ckpt runs/TIMESTAMP/BEST_RUN/model.pt \
#        --generate \
#        --prompt "The meaning of" \
#        --max_tokens 100
#
# 4. ANALYZE TREE EXPANSION:
#    docker run -it --gpus all \
#      -v ${PWD}/runs:/app/runs \
#      boenet:cuda python infer_boenet.py \
#        --ckpt runs/TIMESTAMP/BEST_RUN/model.pt \
#        --debug_policy \
#        --node_samples 100
#
# WHAT TO LOOK FOR:
#   - Does the model produce coherent text?
#   - Is output better than random characters?
#   - Does it capture word patterns, punctuation, structure?
#   - Even with 83K params, the tree structure may help
#
# ============================================================================

# ============================================================================
# FUTURE DEVELOPMENT: DYNAMIC DEPTH ALGORITHM
# ============================================================================
#
# If this sweep shows depth matters, the next step is DYNAMIC depth:
#
# CURRENT (Fixed Depth):
#   ```python
#   for depth in range(max_depth):
#       expand_all_nodes_at_depth(depth)
#   ```
#
# FUTURE (Dynamic Depth):
#   ```python
#   depth = 0
#   while should_expand(current_state) and depth < max_depth:
#       expand_nodes_at_depth(depth)
#       depth += 1
#   ```
#
# IMPLEMENTATION OPTIONS:
#
# Option A: Confidence-based stopping
#   - Compute prediction confidence at each depth
#   - Stop when confidence > threshold
#   - Simple, interpretable
#
# Option B: Learned stopping policy
#   - Add "stop" action to policy network
#   - Policy outputs: [grow_prob, stop_prob]
#   - Train with REINFORCE
#   - More flexible, harder to train
#
# Option C: Budget-based stopping
#   - Set compute budget per input
#   - Expand until budget exhausted
#   - Guarantees worst-case latency
#
# RESEARCH QUESTIONS:
#   1. Does dynamic depth improve quality?
#   2. Does it reduce compute for easy inputs?
#   3. Can we learn when to think deeper?
#   4. How does this compare to transformers?
#
# This is the path toward TRUE adaptive reasoning:
#   - Not just fixed trees (K, depth)
#   - But LEARNED computation allocation
#   - Model decides how much to think
#
# ============================================================================

# ============================================================================
# SUMMARY: THE JOURNEY SO FAR
# ============================================================================
#
# PHASE 0: Original 400-cell sweep (crashed)
#   - K=3 CUDA assertion error
#   - Fixed with v1.1.0/v2.0.1 stability patches
#
# PHASE 1A: K=0 baseline (16 runs)
#   - PPL: 11.55
#   - Latency: 0.24 ms
#   - Fast but no tree structure
#
# PHASE 1B: K=3 baseline (10 runs)
#   - PPL: 11.63
#   - Latency: 1.78 ms
#   - Slower, slightly worse than K=0
#
# PHASE 1C: K=2 vs K=4 comparison (11 runs)
#   - K=2: PPL 11.64, Latency 0.89ms, 79% sparsity ← WINNER
#   - K=4: PPL 11.70, Latency 2.87ms, sparsity hurts quality
#
# PHASE 2 (THIS SWEEP): Depth comparison
#   - Testing: depth=2,3,4 for K=2
#   - Hypothesis: More depth → Better PPL
#   - Goal: Match K=0 (11.55) with tree structure
#
# PHASE 3 (NEXT): User interaction
#   - Test best model with real prompts
#   - See if tree structure helps coherence
#
# PHASE 4 (FUTURE): Dynamic depth algorithm
#   - Model decides when to expand deeper
#   - True adaptive reasoning
#   - The real innovation
#
# ============================================================================

# ============================================================================
# ALL PREVIOUS DATA FOR REFERENCE
# ============================================================================
#
# K=0 BASELINE (from original sweep):
# | Epochs | Val PPL | Latency  |
# |--------|---------|----------|
# | 5      | 11.60   | 0.24 ms  |
# | 10     | 11.58   | 0.24 ms  |
# | 15     | 11.56   | 0.24 ms  |
# | 20     | 11.55   | 0.24 ms  |
#
# K=2 DEPTH=2 (from Phase 1):
# | Epochs | λ    | Thr  | Val PPL | Nodes | Latency | Sparsity |
# |--------|------|------|---------|-------|---------|----------|
# | 10     | 0.0  | 0.40 | 11.69   | 7.0   | 1.35 ms | 0%       |
# | 15     | 0.0  | 0.40 | 11.66   | 7.0   | 1.31 ms | 0%       |
# | 10     | 0.0  | 0.50 | 11.68   | 2.07  | 1.04 ms | 70.5%    |
# | 15     | 0.0  | 0.50 | 11.64   | 1.61  | 0.95 ms | 77.1%    |
# | 10     | 0.05 | 0.40 | 11.70   | 7.0   | 1.34 ms | 0%       |
# | 15     | 0.05 | 0.40 | 11.66   | 7.0   | 1.31 ms | 0%       |
# | 10     | 0.05 | 0.50 | 11.68   | 1.96  | 1.02 ms | 72.0%    |
# | 15     | 0.05 | 0.50 | 11.64   | 1.47  | 0.89 ms | 79.1%    |
#
# K=3 DEPTH=2 (from original sweep):
# | Epochs | Val PPL | Nodes | Latency  |
# |--------|---------|-------|----------|
# | 10     | 11.69   | 13.0  | 1.78 ms  |
# | 15     | 11.65   | 13.0  | 1.78 ms  |
# | 20     | 11.63   | 13.0  | 1.78 ms  |
#
# K=4 DEPTH=2 (from Phase 1):
# | Epochs | Thr  | Val PPL | Nodes | Latency  |
# |--------|------|---------|-------|----------|
# | 10     | 0.40 | 11.74   | 21.0  | 3.75 ms  |
# | 15     | 0.40 | 11.70   | 21.0  | 3.72 ms  |
# | 10     | 0.50 | 11.91   | 7.23  | 2.87 ms  | ← Sparsity hurts!
#
# ============================================================================