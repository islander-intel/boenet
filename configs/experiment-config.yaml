# ============================================================================
# BoeNet v4.0.0 - SCALED BPE MODEL CONFIGURATION
# ============================================================================
#
# PURPOSE: Train BoeNet with BPE tokenization (cl100k_base) for real
#          language modeling quality. This addresses the core issue that
#          character-level models are too small for meaningful generation.
#
# ============================================================================
# THE UPGRADE: CHARACTER-LEVEL â†’ BPE TOKENIZATION
# ============================================================================
#
# WHAT WE'RE FIXING:
# ------------------
# The v3.2.0 comprehensive sweep showed that:
# 1. The model architecture works correctly
# 2. The policy learns to not expand (correctly for the task)
# 3. But the generated text is gibberish because:
#    - Character-level tokenization (vocab=256)
#    - Tiny model (~150K parameters)
#    - Simple next-character prediction doesn't benefit from trees
#
# THE SOLUTION:
# -------------
# Scale up to a real language model:
#
# | Parameter     | v3.2.0 (char) | v4.0.0 (BPE)    |
# |---------------|---------------|-----------------|
# | vocab_size    | 256           | 100,277         |
# | embed_dim     | 64            | 256             |
# | hidden_dim    | 128           | 512             |
# | Parameters    | ~150K         | ~78M            |
# | Tokenizer     | Character     | cl100k_base     |
#
# ============================================================================
# TOKENIZER: cl100k_base (GPT-4 TOKENIZER)
# ============================================================================
#
# We use tiktoken's cl100k_base encoding:
# - Used by GPT-4 and ChatGPT
# - 100,277 vocabulary size
# - ~30% better compression than GPT-2
# - Modern BPE with byte fallback
#
# COMPARISON:
# | Encoding      | Vocab Size | Tokens/Char | Notes                |
# |---------------|------------|-------------|----------------------|
# | char          | 256        | 1.0         | One token per char   |
# | gpt2          | 50,257     | ~0.25       | GPT-2 tokenizer      |
# | cl100k_base   | 100,277    | ~0.22       | GPT-4 tokenizer      |
#
# ============================================================================
# PARAMETER CALCULATION
# ============================================================================
#
# v4.0.0 Model with BPE:
#
# | Component         | Calculation                | Params     |
# |-------------------|----------------------------|------------|
# | Embedding         | 100,277 Ã— 256              | 25.7M      |
# | Input projection  | 256 Ã— 512 + 512            | 131K       |
# | Node transform    | 512 Ã— 512 + 512            | 262K       |
# | Child proj (k=2)  | 512 Ã— 1024 + 1024          | 525K       |
# | Policy network    | 512 Ã— 64 + 64 + 64 + 1     | ~33K       |
# | Output head       | 512 Ã— 100,277 + 100,277    | 51.4M      |
# | TOTAL             |                            | ~78M       |
#
# For comparison, v3.2.0 character-level was ~150K parameters.
# This is a 520x scale-up!
#
# ============================================================================

# ============================================================================
# SWEEP CONFIGURATION
# ============================================================================
sweep:
  # ==========================================================================
  # v4.0.0: TOKENIZER SETTINGS (NEW)
  # ==========================================================================
  
  # Tokenizer type: "bpe" for BPE (cl100k_base), "char" for character-level
  tokenizer_type: bpe
  
  # BPE encoding: cl100k_base (GPT-4), gpt2, r50k_base, p50k_base
  bpe_encoding: cl100k_base
  
  # Vocabulary size is determined by tokenizer:
  # - char: 256
  # - gpt2: 50,257
  # - cl100k_base: 100,277
  # Do NOT set vocab_size manually - it's auto-detected from tokenizer
  
  # ==========================================================================
  # v4.0.0: SCALED MODEL DIMENSIONS
  # ==========================================================================
  
  # Embedding dimension: Scaled up for BPE
  # - char: 64
  # - BPE: 256
  embed_dim_list: [64]
  
  # Hidden dimension: Scaled up for BPE
  # - char: 128
  # - BPE: 512
  hidden_dims: [644]
  
  # Sequence length: Can be shorter with BPE (fewer tokens per word)
  seq_len_list: [128]
  
  # Dataset: WikiText-2 (consistent with all previous runs)
  dataset: wikitext2
  
  # ==========================================================================
  # K VALUES - FOCUSED SWEEP
  # ==========================================================================
  #
  # Based on v3.2.0 findings:
  # - k=0 (dense) is the baseline
  # - k=2 (binary) is most efficient
  # - Higher k values showed diminishing returns
  #
  # For the scaled model, we focus on k=0 and k=2:
  k_values: [2]
  
  # ==========================================================================
  # DEPTH VALUES - FOCUSED SWEEP
  # ==========================================================================
  #
  # v3.2.0 showed that depth matters but has diminishing returns.
  # For the scaled model, test depths 0, 2, 4:
  #
  # | Depth | Nodes | Reasoning Levels |
  # |-------|-------|------------------|
  # | 0     | 1     | Root only        |
  # | 2     | 7     | 3 levels         |
  # | 4     | 31    | 5 levels         |
  #
  max_depths: [4]
  
  # ==========================================================================
  # EPOCHS - REDUCED FOR LARGER MODEL
  # ==========================================================================
  #
  # The 78M parameter model will take longer per epoch.
  # Start with fewer epochs to get initial results faster.
  #
  epochs_list: [30]
  
  # ==========================================================================
  # POLICY PARAMETERS - PROVEN VALUES
  # ==========================================================================
  
  # Lambda efficiency: 0.05 (best from Phase 1)
  lambda_efficiency_list: [0.05]
  
  # Greedy threshold: 0.50
  greedy_threshold_list: [0.50]
  
  # Number of rollouts: 3
  num_rollouts_list: [3]
  
  # Entropy bonus: 0.01
  beta_entropy_list: [0.01]
  
  # Policy loss weight: 0.5
  beta_policy_list: [0.5]
  
  # Epsilon-greedy exploration: 10%
  min_explore_prob: 0.10
  
  # ==========================================================================
  # ARCHITECTURE PARAMETERS
  # ==========================================================================
  
  # Pooling mode: mean
  poolings: [mean]
  
  # ==========================================================================
  # v4.0.0: OPTIMIZATION - ADJUSTED FOR LARGER MODEL
  # ==========================================================================
  
  # Learning rate: Lower for larger model
  # - char: 0.001
  # - BPE: 0.0001
  lrs: [0.0001]
  
  # Batch size: Smaller for larger model (memory constraints)
  # - char: 64
  # - BPE: 32 or 16
  batch_sizes: [16]
  
  # Weight decay: Add regularization for larger model
  # - char: 0.0
  # - BPE: 0.01
  weight_decays: [0.01]
  
  # Gradient clipping: Keep same
  grad_clip: 1.0
  
  # Policy gradient clipping: Keep same
  policy_grad_clip: 0.5

# ============================================================================
# TRAINING SETTINGS
# ============================================================================
training:
  # Default epochs
  epochs: 10
  
  # Single run per config
  repeats: 1
  
  # Random seed
  seed0: 42
  
  # Dataset
  dataset: wikitext2
  
  # LR schedule
  lr_schedule: cosine
  
  # Optimizer
  optimizer: adamw

# ============================================================================
# INFERENCE SETTINGS
# ============================================================================
inference:
  # Number of samples for perplexity measurement
  infer_samples: 1000
  
  # Force CPU inference for fair comparison
  cpu_only: false
  
  # Enable policy analysis
  debug_policy: true

# ============================================================================
# PATHS
# ============================================================================
paths:
  # Output directory
  save_root: runs
  
  # Dataset directory
  data_root: ./data
  
  # Training script (v4.0.0)
  train_script: train_boenet.py
  
  # Inference script
  infer_script: infer_boenet.py

# ============================================================================
# RUN COUNT CALCULATION
# ============================================================================
#
# FOCUSED SWEEP:
#
# k=0 (depth ignored):
#   1 config Ã— 3 epochs = 3 runs
#
# k=2:
#   3 depths Ã— 3 epochs = 9 runs
#
# TOTAL: 3 + 9 = 12 runs
#
# TIME ESTIMATE:
#   - ~78M parameters, expect ~15-30 min per epoch
#   - Average ~10 min per run with early epochs
#   - 12 runs Ã— 10 min = ~2 hours for initial sweep
#
# ============================================================================

# ============================================================================
# EXPECTED IMPROVEMENTS WITH BPE
# ============================================================================
#
# 1. TEXT QUALITY:
#    - Coherent words and sentences (not gibberish)
#    - Proper punctuation and spacing
#    - Reasonable grammar
#
# 2. PERPLEXITY:
#    - Random baseline: 100,277 (vocab size)
#    - Expected after training: 100-500 (language model range)
#    - Much more meaningful than char-level PPL
#
# 3. TREE UTILIZATION:
#    - More complex token relationships
#    - Policy may actually learn when to expand
#    - Tree structure might provide real benefit
#
# ============================================================================

# ============================================================================
# USAGE INSTRUCTIONS
# ============================================================================
#
# PREREQUISITES:
#   1. Ensure v4.0.0 files are deployed:
#      - boenet/tokenizer.py (v1.1.0 with cl100k_base)
#      - train_boenet.py (v4.0.0 with BPE support)
#
#   2. Install tiktoken:
#      pip install tiktoken
#
#   3. Clear Python bytecode cache:
#      find . -type d -name __pycache__ -exec rm -rf {} +
#
# START SWEEP:
#   docker run -d --gpus all --name boenet_bpe \
#     -v ${PWD}/data:/app/data \
#     -v ${PWD}/runs:/app/runs \
#     -v ${PWD}/configs:/app/configs \
#     -v ${PWD}/boenet:/app/boenet \
#     boenet:cuda python boenet_training_matrix.py \
#       --config configs/experiment-config.yaml
#
# SINGLE RUN (QUICK TEST):
#   docker run -it --gpus all \
#     -v ${PWD}/data:/app/data \
#     -v ${PWD}/runs:/app/runs \
#     -v ${PWD}/boenet:/app/boenet \
#     boenet:cuda python train_boenet.py \
#       --tokenizer_type bpe \
#       --embed_dim 256 \
#       --hidden_dim 512 \
#       --max_depth 2 \
#       --epochs 5
#
# MONITOR:
#   docker logs -f boenet_bpe
#   nvidia-smi -l 5
#
# ============================================================================

# ============================================================================
# COMPARISON: CHAR vs BPE
# ============================================================================
#
# For scientific comparison, you can run both:
#
# CHARACTER-LEVEL (v3.2.0 baseline):
#   python train_boenet.py \
#       --tokenizer_type char \
#       --embed_dim 64 \
#       --hidden_dim 128 \
#       --epochs 30
#
# BPE (v4.0.0 scaled):
#   python train_boenet.py \
#       --tokenizer_type bpe \
#       --embed_dim 256 \
#       --hidden_dim 512 \
#       --epochs 20
#
# EXPECTED RESULTS:
# | Model      | Params | PPL Baseline | Expected PPL | Generation Quality |
# |------------|--------|--------------|--------------|-------------------|
# | Char v3.2.0| 150K   | 256          | ~11.5        | Gibberish         |
# | BPE v4.0.0 | 78M    | 100,277      | ~100-500     | Coherent text     |
#
# ============================================================================

# ============================================================================
# VERSION HISTORY
# ============================================================================
#
# v4.0.0 (2026-01-03) - BPE TOKENIZATION
#   - Added cl100k_base tokenizer (GPT-4 tokenizer)
#   - Scaled model: embed_dim=256, hidden_dim=512
#   - Parameters: ~78M (vs ~150K for char)
#   - Lower learning rate: 0.0001 (vs 0.001)
#   - Added weight decay: 0.01
#   - Smaller batch size: 32 (vs 64)
#
# v3.2.0 (2026-01-03) - COMPREHENSIVE SWEEP
#   - Fixed node count metrics
#   - 96 runs across full parameter space
#   - Character-level only
#
# v3.1.0 (2026-01-01) - EPSILON-GREEDY
#   - Added min_explore_prob
#   - Fixed tree expansion during training
#
# ============================================================================

# ============================================================================
# SUMMARY
# ============================================================================
#
# This config upgrades BoeNet from a toy character-level model to a real
# language model with BPE tokenization. Key changes:
#
# 1. TOKENIZER: cl100k_base (GPT-4's tokenizer) instead of character-level
# 2. SCALE: 78M parameters instead of 150K (520x larger)
# 3. DIMENSIONS: embed_dim=256, hidden_dim=512
# 4. OPTIMIZATION: Lower LR, weight decay, smaller batches
#
# With these changes, the model should:
# - Generate coherent text (not gibberish)
# - Have meaningful perplexity values
# - Potentially benefit from tree structure for complex reasoning
#
# LET'S GO! ðŸš€
#
# ============================================================================