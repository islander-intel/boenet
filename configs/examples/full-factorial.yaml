# ============================================================================
# BFSNet Full Factorial Configuration (Comprehensive Sweep)
# ============================================================================
# ⚠️  WARNING: This configuration generates a VERY LARGE number of runs!
# ⚠️  Estimated runs: 50,000+ runs
# ⚠️  Estimated time: Multiple days to weeks (depending on hardware)
#
# This configuration explores the full hyperparameter space for:
# - Complete characterization of BFSNet behavior
# - Publication-quality experiments
# - Finding optimal configurations
#
# Consider using a subset or running on a compute cluster.
#
# Usage:
#   python bfs_training_matrix.py --config configs/examples/full-factorial.yaml \
#       --infer_script infer_fmnist_bfs.py
#
# Recommended: Start with small-sweep.yaml to identify promising regions,
# then create a focused config based on those results.
# ============================================================================

sweep:
  # --------------------------------------------------------------------------
  # Model Architecture (Full Range)
  # --------------------------------------------------------------------------
  
  # Full range of K values
  # K=0: Dense baseline (MLP)
  # K=1-5: Increasing branching complexity
  k_values: [0, 1, 2, 3, 4, 5]
  
  # Multiple depths
  # Depth 1: Shallow (root + 1 level)
  # Depth 2-3: Typical range
  # Depth 4: Deep (expensive)
  max_depths: [1, 2, 3, 4]
  
  # Wide range of hidden dimensions
  hidden_dims: [32, 64, 128, 256, 512]
  
  # All pooling modes
  poolings: [learned, sum, mean]
  
  # --------------------------------------------------------------------------
  # Temperature (Full Range)
  # --------------------------------------------------------------------------
  
  # Both schedules
  temp_schedules: [cosine, none]
  
  # Multiple fixed temperatures
  fixed_temps: [0.5, 0.8, 1.0, 1.2, 1.5, 2.0]
  
  # Multiple cosine annealing schedules
  cosine_anneal:
    - "2.0->0.5"     # Aggressive annealing
    - "1.8->0.8"     # Strong annealing
    - "1.6->1.0"     # Moderate annealing
    - "1.4->1.0"     # Light annealing
    - "1.2->0.8"     # Subtle annealing
    - "1.0->1.0"     # No annealing (constant)
  
  # --------------------------------------------------------------------------
  # Optimization (Full Range)
  # --------------------------------------------------------------------------
  
  # Wide range of learning rates
  lrs: [0.0001, 0.0005, 0.001, 0.0025, 0.005, 0.01]
  
  # Multiple batch sizes
  batch_sizes: [32, 64, 128, 256]
  
  # Weight decay options
  weight_decays: [0.0, 0.001, 0.01, 0.1]
  
  # --------------------------------------------------------------------------
  # Training Schedule (Full Range)
  # --------------------------------------------------------------------------
  
  # Multiple warmup settings
  # 0: No warmup (start sparse immediately)
  # 1-2: Minimal warmup
  # 3-5: Standard warmup
  # 7-10: Extended warmup
  warmup_epochs_list: [0, 1, 2, 3, 5, 7, 10]
  
  # Optionally sweep over epochs
  # Uncomment to study convergence at different epoch counts
  # epochs_list: [5, 10, 15, 20, 25, 30]

# ============================================================================
# Training Settings
# ============================================================================
training:
  # Extended training for full characterization
  epochs: 25
  
  # Multiple repeats for statistical significance
  # Consider using 5 for publication-quality results
  repeats: 3
  
  # Base seed
  seed0: 42

# ============================================================================
# Inference Settings
# ============================================================================
inference:
  # Full test set evaluation
  infer_samples: 10000  # Full FashionMNIST test set
  
  # CPU for fair latency comparison
  cpu_only: true
  
  # Sparse inference (production mode)
  infer_force_exec: sparse

# ============================================================================
# Paths
# ============================================================================
paths:
  # Output directory
  save_root: runs/full_factorial
  
  # Dataset
  data_root: ./data
  
  # Scripts
  train_script: train_fmnist_bfs.py
  infer_script: infer_fmnist_bfs.py

# ============================================================================
# Run Estimation
# ============================================================================
# 
# Approximate calculation:
# 
# For K=0 (warmup forced to 0):
#   1 k_value × 4 depths × 5 hidden × 3 poolings × 
#   (6 fixed_temps + 6 cosine_pairs) × 6 lrs × 4 batches × 4 wds × 
#   1 warmup × 3 repeats
#   = 1 × 4 × 5 × 3 × 12 × 6 × 4 × 4 × 1 × 3 = 103,680 runs
#
# For K>0:
#   5 k_values × 4 depths × 5 hidden × 3 poolings ×
#   (6 fixed_temps + 6 cosine_pairs) × 6 lrs × 4 batches × 4 wds ×
#   7 warmups × 3 repeats
#   = 5 × 4 × 5 × 3 × 12 × 6 × 4 × 4 × 7 × 3 = 3,628,800 runs
#
# TOTAL: ~3.7 million runs (!)
#
# This is why you should:
# 1. Start with small-sweep.yaml or test-config.yaml
# 2. Identify promising hyperparameter regions
# 3. Create a focused config for those regions
# 4. Only run full factorial on a large compute cluster
# ============================================================================