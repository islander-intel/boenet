
===============================================================================
⚠️  WARNING: Default greedy_threshold=0.5 May Result in Root-Only Inference!
===============================================================================
ISSUE:
  The policy typically learns grow_prob values around 0.40-0.50 during
  training. With the default greedy threshold of 0.5, most decisions
  will FAIL the threshold check, resulting in ZERO children created.

EXPECTED BEHAVIOR:
  - Training mode: Uses ~6-12 nodes (stochastic Bernoulli sampling)
  - Inference mode: Uses 1 node (root-only, greedy threshold blocks growth)

RECOMMENDATION:
  1. After training, run with --debug_policy to measure learned grow_prob:
     python3 infer_fmnist_bfs.py --ckpt runs/20251220_025958/k3_poolmean_hd64_lr0p001_bs64_wd0p0_d2_lam0p05_thr0p5_roll3_rep0/k3_poolmean_hd64_lr0p001_bs64_wd0p0_d2_lam0p05_thr0p5_roll3_rep0.pt --debug_policy --cpu

  2. Then retrain with threshold ≈ mean_grow_prob - 0.03:
     Example: If mean_grow_prob = 0.445, use --greedy_threshold 0.42

  3. Or use training matrix to sweep (λ, threshold) pairs:
     python3 bfs_training_matrix.py --config configs/threshold_sweep.yaml

See docs/architecture.md for detailed analysis.
===============================================================================

[sanity:fashionmnist] batch=64x784 dtype=float32 xmin=-0.424 xmax=2.821 y[min,max]=[0,9] label_counts_head=(0:11, 1:5, 2:4, 3:3, 4:3), assert_flatten=OK
BFSNet v2.0.0 (
  input_dim=784, hidden_dim=64, output_dim=10,
  max_depth=2, max_children=3,
  greedy_threshold=0.5000,
  sibling_embed=True, sibling_scale=0.1250,
  use_pruning=False, pruning_mode='learned',
  pooling_mode='mean'
)
[dims] FashionMNIST: input_dim=784, num_classes=10
[run] epochs=10 | batch_size=64 | lr=0.001 | wd=0.0 | grad_clip=1.0
      policy: num_rollouts=3, λ_eff=0.05, β_ent=0.01, β_policy=0.5
      greedy_threshold=0.5 (inference)
      pooling_mode=mean
      pruning losses: L1*=0.0, KL*=0.0 (keep_rate=0.5)
[node metrics] Theoretical max nodes per example per rollout: 13
               With 3 rollouts: 39 total
               Batch-level max (B=64): 832 per rollout
[opt] adamw | lr=0.001 | wd=0.0
[lr] schedule=cosine

[epoch 1/10] lr=0.001000
[probe e=1 b=0] loss=0.818869
  logits: mean=0.0625 std=0.1683 min=-0.7453 max=0.6447
  root_fc.weight: grad_norm=1.015409e+00
  output_fc.weight: grad_norm=2.599985e-01
  ||root_fc||_2=7.311425e-01
  ||output_fc||_2=7.463145e-01
[probe e=1 b=1] loss=0.880954
  logits: mean=0.0497 std=0.2315 min=-0.8459 max=1.7519
  root_fc.weight: grad_norm=6.600024e-01
  output_fc.weight: grad_norm=2.586936e-01
  ||root_fc||_2=7.321495e-01
  ||output_fc||_2=7.448361e-01
  Train Loss: -1.1129 (cls=0.5436, policy=-3.3129) | Val Loss: 0.4343
  Train Acc: 80.88% | Val Acc: 84.25% | Avg Total Nodes/Example: 14.08
  (Sparsity: 36.11% of theoretical max 39 nodes)

[epoch 2/10] lr=0.000976
[probe e=2 b=0] loss=-0.777283
  logits: mean=-1.5734 std=4.3318 min=-10.4926 max=12.2150
  root_fc.weight: grad_norm=5.429317e-01
  output_fc.weight: grad_norm=1.022182e+00
  ||root_fc||_2=3.132806e+00
  ||output_fc||_2=1.300565e+00
[probe e=2 b=1] loss=-0.918561
  logits: mean=-1.4930 std=4.1522 min=-11.5216 max=13.2174
  root_fc.weight: grad_norm=4.359455e-01
  output_fc.weight: grad_norm=8.555369e-01
  ||root_fc||_2=3.132027e+00
  ||output_fc||_2=1.301176e+00
  Train Loss: -1.2895 (cls=0.3771, policy=-3.3332) | Val Loss: 0.4011
  Train Acc: 86.18% | Val Acc: 85.85% | Avg Total Nodes/Example: 14.06
  (Sparsity: 36.06% of theoretical max 39 nodes)

[epoch 3/10] lr=0.000905
  Train Loss: -1.3203 (cls=0.3365, policy=-3.3135) | Val Loss: 0.3851
  Train Acc: 87.55% | Val Acc: 85.93% | Avg Total Nodes/Example: 13.99
  (Sparsity: 35.88% of theoretical max 39 nodes)

[epoch 4/10] lr=0.000794
  Train Loss: -1.3612 (cls=0.3073, policy=-3.3371) | Val Loss: 0.3640
  Train Acc: 88.70% | Val Acc: 86.85% | Avg Total Nodes/Example: 14.04
  (Sparsity: 36.00% of theoretical max 39 nodes)

[epoch 5/10] lr=0.000655
  Train Loss: -1.3835 (cls=0.2834, policy=-3.3339) | Val Loss: 0.3558
  Train Acc: 89.54% | Val Acc: 87.03% | Avg Total Nodes/Example: 14.12
  (Sparsity: 36.21% of theoretical max 39 nodes)

[epoch 6/10] lr=0.000500
  Train Loss: -1.4052 (cls=0.2647, policy=-3.3397) | Val Loss: 0.3517
  Train Acc: 90.11% | Val Acc: 87.30% | Avg Total Nodes/Example: 14.09
  (Sparsity: 36.13% of theoretical max 39 nodes)

[epoch 7/10] lr=0.000345
  Train Loss: -1.4217 (cls=0.2464, policy=-3.3362) | Val Loss: 0.3373
  Train Acc: 90.85% | Val Acc: 87.83% | Avg Total Nodes/Example: 14.06
  (Sparsity: 36.06% of theoretical max 39 nodes)

[epoch 8/10] lr=0.000206
  Train Loss: -1.4258 (cls=0.2324, policy=-3.3163) | Val Loss: 0.3361
  Train Acc: 91.46% | Val Acc: 88.08% | Avg Total Nodes/Example: 14.11
  (Sparsity: 36.17% of theoretical max 39 nodes)

[epoch 9/10] lr=0.000095
  Train Loss: -1.4474 (cls=0.2224, policy=-3.3397) | Val Loss: 0.3306
  Train Acc: 91.88% | Val Acc: 88.05% | Avg Total Nodes/Example: 14.14
  (Sparsity: 36.26% of theoretical max 39 nodes)

[epoch 10/10] lr=0.000024
  Train Loss: -1.4460 (cls=0.2158, policy=-3.3235) | Val Loss: 0.3276
  Train Acc: 92.14% | Val Acc: 88.22% | Avg Total Nodes/Example: 14.13
  (Sparsity: 36.24% of theoretical max 39 nodes)

Saved checkpoint → runs/20251220_025958/k3_poolmean_hd64_lr0p001_bs64_wd0p0_d2_lam0p05_thr0p5_roll3_rep0/k3_poolmean_hd64_lr0p001_bs64_wd0p0_d2_lam0p05_thr0p5_roll3_rep0.pt
Best val loss: 0.3276 at epoch 10

===============================================================================
POST-TRAINING: Greedy Threshold Tuning Recommendations
===============================================================================

Your model was trained with:
  lambda_efficiency = 0.05
  greedy_threshold  = 0.5

NEXT STEPS:

1. Measure the learned grow_prob distribution:
   python3 infer_fmnist_bfs.py --ckpt runs/20251220_025958/k3_poolmean_hd64_lr0p001_bs64_wd0p0_d2_lam0p05_thr0p5_roll3_rep0/k3_poolmean_hd64_lr0p001_bs64_wd0p0_d2_lam0p05_thr0p5_roll3_rep0.pt \
       --debug_policy --node_samples 1000 --cpu

   This will show you:
   - Mean grow_prob (typically 0.40-0.50)
   - % of decisions above threshold 0.5
   - Distribution histogram

2. Based on the measured mean_grow_prob, choose optimal threshold:

   If mean_grow_prob ≈ 0.445, recommended settings:
     • threshold = 0.50 → Root-only (1 node, max efficiency)
     • threshold = 0.42 → Balanced (~6-8 nodes, good accuracy)
     • threshold = 0.35 → Dense (~10-12 nodes, max accuracy)

   General rule: optimal_threshold ≈ mean_grow_prob - 0.03

3. Retrain with optimal threshold (if needed):
   python3 train_fmnist_bfs.py \
       --lambda_efficiency 0.05 \
       --greedy_threshold 0.42 \
       --epochs 10

4. OR run full (λ, threshold) sweep to find optimal pair:
   python3 bfs_training_matrix.py \
       --config configs/threshold_sweep.yaml

EMPIRICAL FINDINGS:
  • Higher λ (0.05) often gives BETTER accuracy than lower λ (0.01)
  • Policy learns grow_prob ≈ 0.44-0.45 regardless of λ value
  • Default threshold=0.5 typically results in root-only inference
  • Root-only achieves ~86-87% accuracy on FashionMNIST

See docs/architecture.md for complete analysis and recommendations.
===============================================================================

__SUMMARY__ {"run": {"seed": 42, "epochs": 10, "batch_size": 64}, "optimizer": {"name": "adamw", "lr": 0.001, "weight_decay": 0.0}, "model": {"hidden_dim": 64, "max_depth": 2, "max_children": 3, "pooling_mode": "mean", "greedy_threshold": 0.5}, "policy": {"num_rollouts": 3, "lambda_efficiency": 0.05, "beta_entropy": 0.01, "beta_policy": 0.5}, "results": {"best_val_loss": 0.32762847499052683, "best_epoch": 10, "final_val_acc": 88.21666666666667}, "time": {"total_s": 221.19668680100585, "epoch_avg_s": 22.11966749659623, "last_epoch_s": 24.446693336009048}, "artifacts": {"checkpoint": "runs/20251220_025958/k3_poolmean_hd64_lr0p001_bs64_wd0p0_d2_lam0p05_thr0p5_roll3_rep0/k3_poolmean_hd64_lr0p001_bs64_wd0p0_d2_lam0p05_thr0p5_roll3_rep0.pt"}}
