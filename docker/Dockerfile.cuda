# ============================================================================
# BoeNet CUDA Dockerfile (v1.1.0 - BPE Tokenizer Support)
# ============================================================================
# GPU-enabled container for accelerated training and inference.
#
# v1.1.0 Changes (2026-01-03):
# ----------------------------
#   - ADDED: tiktoken for BPE tokenization (cl100k_base, GPT-4 tokenizer)
#   - ADDED: tiktoken verification step during build
#   - UPDATED: Environment variables for BPE tokenization defaults
#   - UPDATED: Labels for v4.0.0 training script compatibility
#   - This enables word/subword-level tokenization instead of character-level
#   - Required for train_boenet.py v4.0.0 with --tokenizer_type bpe
#
# Converted from BFSNet (Vision) to BoeNet (Language)
# ---------------------------------------------------
# Key Changes:
#   - RENAMED: All bfsnet references → boenet
#   - RENAMED: train_fmnist_bfs.py → train_boenet.py
#   - RENAMED: infer_fmnist_bfs.py → infer_boenet.py
#   - RENAMED: bfs_training_matrix.py → boenet_training_matrix.py
#   - RENAMED: Environment variables BFSNET_* → BOENET_*
#   - ADDED: datasets library for HuggingFace Shakespeare/TinyStories
#   - ADDED: tiktoken for BPE tokenization (v1.1.0)
#   - UPDATED: Labels and descriptions for language model
#   - UNCHANGED: All CUDA/PyTorch configuration
#
# Build:
#   docker build -t boenet:cuda -f docker/Dockerfile.cuda .
#
# Run (foreground):
#   docker run --rm --gpus all \
#       -v $(pwd)/data:/app/data -v $(pwd)/runs:/app/runs \
#       boenet:cuda python train_boenet.py --epochs 10 --dataset wikitext2
#
# Run with BPE Tokenization (v1.1.0 / v4.0.0):
#   docker run --rm --gpus all \
#       -v $(pwd)/data:/app/data -v $(pwd)/runs:/app/runs \
#       boenet:cuda python train_boenet.py \
#           --tokenizer_type bpe \
#           --embed_dim 256 \
#           --hidden_dim 512 \
#           --epochs 10
#
# Run in Background (detached mode - survives terminal close/sleep):
#   docker run -d --gpus all \
#       --name boenet_training \
#       -v $(pwd)/data:/app/data \
#       -v $(pwd)/runs:/app/runs \
#       -v $(pwd)/configs:/app/configs \
#       boenet:cuda python boenet_training_matrix.py \
#           --config configs/experiment-config.yaml \
#           --infer_script infer_boenet.py
#
# Monitor Background Run:
#   docker logs -f boenet_training         # Follow live output
#   docker logs --tail 50 boenet_training  # Last 50 lines
#   docker ps                              # Check if still running
#
# Stop Background Run:
#   docker stop boenet_training            # Graceful stop
#   docker rm boenet_training              # Remove container
#   docker rm -f boenet_training           # Force stop and remove
#
# Text Generation (interactive):
#   docker run --rm -it --gpus all \
#       -v $(pwd)/runs:/app/runs \
#       boenet:cuda python infer_boenet.py \
#           --ckpt runs/checkpoint.pt \
#           --generate --prompt "Once upon a time" \
#           --max_tokens 200 --temperature 0.8
#
# Multi-GPU:
#   docker run --rm --gpus '"device=0,1"' \
#       -v $(pwd)/data:/app/data -v $(pwd)/runs:/app/runs \
#       boenet:cuda python train_boenet.py --dataset tinystories
#
# Interactive:
#   docker run --rm -it --gpus all -v $(pwd):/app boenet:cuda bash
#
# Requirements:
#   - NVIDIA GPU with CUDA support
#   - NVIDIA Container Toolkit (nvidia-docker2)
#   - Docker 19.03+ with GPU support
#   - NVIDIA Driver with CUDA >= 12.8 support (driver 570.x+ recommended)
#
# Supported GPU Architectures:
#   - Pascal (GTX 10 series) - Compute Capability 6.x (sm_60, sm_61)
#   - Volta (Titan V, Quadro GV100) - Compute Capability 7.0 (sm_70)
#   - Turing (RTX 20 series) - Compute Capability 7.5 (sm_75)
#   - Ampere (RTX 30 series, A100) - Compute Capability 8.x (sm_80, sm_86)
#   - Ada Lovelace (RTX 40 series) - Compute Capability 8.9 (sm_89)
#   - Hopper (H100) - Compute Capability 9.0 (sm_90)
#   - Blackwell (RTX 50 series) - Compute Capability 12.0 (sm_120)
#
# Note on CUDA Compatibility:
#   This container uses CUDA 12.8 toolkit which is REQUIRED for Blackwell GPUs.
#   RTX 5080/5090 (Blackwell) uses sm_120 compute capability which needs:
#     - CUDA 12.8+ toolkit
#     - PyTorch 2.7+ with cu128 wheels
#   Your host driver must support CUDA >= 12.8 (driver 570.x or newer).
#
# IMPORTANT - Blackwell (RTX 50 series) Notes:
#   - RTX 5080/5090 use compute capability sm_120 (NOT sm_100 as some docs say)
#   - Standard PyTorch pip packages only support up to sm_90
#   - Must use PyTorch cu128 wheels for sm_120 support
#   - NVIDIA changed Docker image naming: use 'cudnn' NOT 'cudnn9'
#
# Installed Python Packages:
#   - PyTorch 2.7.1 (cu128) - Deep learning framework with Blackwell support
#   - torchvision - Vision utilities (for potential multi-modal extensions)
#   - datasets - HuggingFace datasets for WikiText-2/Shakespeare/TinyStories (REQUIRED)
#   - tiktoken - OpenAI's BPE tokenizer for cl100k_base encoding (REQUIRED for v4.0.0)
#   - tqdm - Progress bars for training matrix
#   - pyyaml - YAML config file support
#   - pandas - Data analysis for results
#   - matplotlib/seaborn - Visualization
#   - scikit-learn - ML utilities
#
# Language Model Specific:
#   - Supports WikiText-2, WikiText-103, Shakespeare, TinyStories, and custom text files
#   - Character-level tokenization (vocab_size=256) - legacy
#   - BPE tokenization with tiktoken (vocab_size=100,277) - NEW v1.1.0/v4.0.0
#   - Configurable sequence length (seq_len) and embedding dimension (embed_dim)
#   - Text generation with temperature, top-k, and top-p sampling
#
# Tokenizer Options (v1.1.0):
#   - Character-level (--tokenizer_type char): vocab_size=256, embed_dim=64, hidden_dim=128
#   - BPE (--tokenizer_type bpe): vocab_size=100,277, embed_dim=256, hidden_dim=512
#   - BPE produces ~78M parameter model vs ~150K for character-level (520x larger)
#
# Changelog:
#   2026-01-03 - v1.1.0 BPE TOKENIZER SUPPORT:
#                - Added tiktoken package for BPE tokenization
#                - Added tiktoken verification step during build
#                - Updated environment variables for BPE defaults
#                - Required for train_boenet.py v4.0.0 / tokenizer.py v1.1.0
#                - Supports cl100k_base (GPT-4), gpt2, and other encodings
#   2025-12-29 - v1.0.3 BUGFIX:
#                - Fixed HuggingFace cache permission error for Docker volume mounts
#                - Changed HF_DATASETS_CACHE and HF_HOME to /app/data/.cache
#                - This allows HuggingFace to write to mounted /app/data volume
#                - Cache persists across container runs when data volume is mounted
#                - Works correctly on both Linux and Windows Docker hosts
#                - Updated default dataset from shakespeare to wikitext2
#   2025-12-22 - v1.0.2 BUGFIX:
#                - Fixed HuggingFace cache permission error
#                - Changed HF_DATASETS_CACHE and HF_HOME to /home/boenet/.cache/huggingface
#                - This avoids permission conflicts with mounted volumes
#   2025-12-22 - v1.0.1 BUGFIX:
#                - Added datasets library for HuggingFace Shakespeare/TinyStories
#                - Added datasets verification step during build
#   2025-12-22 - v1.0.0 LANGUAGE MODEL RELEASE:
#                - Converted from BFSNet (vision) to BoeNet (language)
#                - Updated all script references for language modeling
#                - Added text generation examples
#                - Updated environment variables to BOENET_*
#                - Updated labels and documentation
#   2025-12-15 - Added tqdm verification step, updated documentation for
#                detached mode running, updated labels for new features
#   2025-12-09 - MAJOR UPDATE for Blackwell (RTX 50 series) support:
#                - Changed CUDA from 12.6.2 to 12.8.0 (12.6.2 doesn't exist)
#                - Changed cuDNN tag from 'cudnn9' to 'cudnn' (naming convention changed)
#                - Changed PyTorch from 2.5.1 to 2.7.1 (cu128 required for sm_120)
#                - Changed PyTorch index from cu124 to cu128 (required for Blackwell)
#                - Fixed TORCH_CUDA_ARCH_LIST: sm_120 NOT sm_100 for Blackwell
#   2025-09-18 - Fixed permission issue with directory creation
#   2025-09-18 - Updated to CUDA 12.6 and PyTorch 2.5+ for RTX 50 series
# ============================================================================

# ----------------------------------------------------------------------------
# Build Arguments
# ----------------------------------------------------------------------------
# CUDA 12.8.0 - REQUIRED for Blackwell (RTX 50 series) sm_120 support
# Note: nvidia/cuda:12.6.2-cudnn9-runtime-ubuntu22.04 does NOT exist on Docker Hub
# Available images use format: nvidia/cuda:VERSION-cudnn-runtime-ubuntuVERSION
# (no version number after 'cudnn' - naming convention changed)
ARG CUDA_VERSION=12.8.0
ARG UBUNTU_VERSION=22.04
ARG PYTHON_VERSION=3.11
# PyTorch 2.7.1 with cu128 includes sm_120 (Blackwell) support
ARG PYTORCH_VERSION=2.7.1

# ----------------------------------------------------------------------------
# Base Image - NVIDIA CUDA Runtime
# ----------------------------------------------------------------------------
# Using CUDA 12.8 runtime with cuDNN for RTX 50 series (Blackwell) support
# CRITICAL: Tag format is 'cudnn' NOT 'cudnn9' - NVIDIA changed naming convention
# Note: CUDA toolkit version must be <= host driver's CUDA version
FROM nvidia/cuda:${CUDA_VERSION}-cudnn-runtime-ubuntu${UBUNTU_VERSION} AS base

# Prevent interactive prompts during package installation
ENV DEBIAN_FRONTEND=noninteractive

# Python and system encoding
ENV PYTHONDONTWRITEBYTECODE=1 \
    PYTHONUNBUFFERED=1 \
    PYTHONIOENCODING=UTF-8 \
    LANG=C.UTF-8 \
    LC_ALL=C.UTF-8

# ----------------------------------------------------------------------------
# System Dependencies
# ----------------------------------------------------------------------------
FROM base AS system-deps

# Install Python and system packages
ARG PYTHON_VERSION
RUN apt-get update && apt-get install -y --no-install-recommends \
    # Python - using 3.11 for best compatibility with PyTorch 2.7
    python${PYTHON_VERSION} \
    python${PYTHON_VERSION}-dev \
    python${PYTHON_VERSION}-venv \
    python3-pip \
    # Build essentials
    build-essential \
    gcc \
    g++ \
    # Git
    git \
    # Image processing (for potential multi-modal extensions)
    libjpeg-dev \
    libpng-dev \
    # Scientific computing
    libopenblas-dev \
    liblapack-dev \
    # Utilities
    curl \
    wget \
    ca-certificates \
    && rm -rf /var/lib/apt/lists/* \
    && apt-get clean

# Set Python as default
ARG PYTHON_VERSION
RUN update-alternatives --install /usr/bin/python python /usr/bin/python${PYTHON_VERSION} 1 \
    && update-alternatives --install /usr/bin/python3 python3 /usr/bin/python${PYTHON_VERSION} 1

# ----------------------------------------------------------------------------
# Python Dependencies
# ----------------------------------------------------------------------------
FROM system-deps AS python-deps

# Upgrade pip
RUN pip install --no-cache-dir --upgrade pip setuptools wheel

# Install PyTorch with CUDA 12.8 support (cu128)
# CRITICAL: Must use cu128 index for Blackwell (sm_120) support
# Standard cu124 wheels do NOT include sm_120 kernels
# PyTorch 2.7+ with cu128 includes compute capability 12.0 (Blackwell/RTX 50 series)
ARG PYTORCH_VERSION
RUN pip install --no-cache-dir \
    torch==${PYTORCH_VERSION} \
    torchvision \
    torchaudio \
    --index-url https://download.pytorch.org/whl/cu128

# Verify PyTorch installation and CUDA support
# This will fail the build early if something is wrong
RUN python -c "import torch; print(f'PyTorch version: {torch.__version__}'); print(f'CUDA available: {torch.cuda.is_available()}'); print(f'CUDA version: {torch.version.cuda}')"

# Copy and install project requirements
WORKDIR /app
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Install additional packages with updated versions
# CRITICAL: tqdm is REQUIRED for progress bars in boenet_training_matrix.py
# CRITICAL: datasets is REQUIRED for HuggingFace WikiText-2/Shakespeare/TinyStories
# CRITICAL: tiktoken is REQUIRED for BPE tokenization in train_boenet.py v4.0.0
RUN pip install --no-cache-dir \
    pyyaml>=6.0.0 \
    tqdm>=4.66.0 \
    pandas>=2.0.0 \
    matplotlib>=3.7.0 \
    seaborn>=0.12.0 \
    networkx>=3.0 \
    scipy>=1.11.0 \
    scikit-learn>=1.3.0 \
    datasets>=2.14.0 \
    tiktoken>=0.5.0

# Verify tqdm installation (required for progress bars in boenet_training_matrix.py)
RUN python -c "import tqdm; print(f'tqdm installed successfully, version: {tqdm.__version__}')"

# Verify datasets installation (required for WikiText-2/Shakespeare/TinyStories)
RUN python -c "import datasets; print(f'datasets installed successfully, version: {datasets.__version__}')"

# Verify tiktoken installation (required for BPE tokenization in v4.0.0)
# Also verify that cl100k_base encoding is available
RUN python -c "import tiktoken; enc = tiktoken.get_encoding('cl100k_base'); print(f'tiktoken installed successfully, version: {tiktoken.__version__}'); print(f'cl100k_base vocab size: {enc.n_vocab}')"

# Optional: Install NVIDIA tools for monitoring
RUN pip install --no-cache-dir \
    nvidia-ml-py3 \
    gpustat

# Optional: Install pytest-env for environment variable support in tests
RUN pip install --no-cache-dir pytest-env

# ----------------------------------------------------------------------------
# Application
# ----------------------------------------------------------------------------
FROM python-deps AS app

# Create non-root user for security
RUN useradd --create-home --shell /bin/bash boenet

# Set working directory (still running as root at this point)
WORKDIR /app

# ----------------------------------------------------------------------------
# Directory Creation and Permissions (CRITICAL FOR HUGGINGFACE)
# ----------------------------------------------------------------------------
# Create necessary directories AS ROOT (before switching to boenet user)
# This ensures we have permission to create all directories
#
# CRITICAL FIX (v1.0.3): Create /app/data/.cache for HuggingFace
# 
# The HuggingFace datasets library needs a writable cache directory.
# By placing it under /app/data/.cache, it will:
#   1. Be writable when /app/data is mounted from the host
#   2. Persist across container runs (datasets are cached)
#   3. Work correctly on both Linux and Windows Docker hosts
#
# Directory structure:
#   /app/data/          - Data directory (mounted from host)
#   /app/data/.cache/   - HuggingFace cache (inside mounted volume)
#   /app/runs/          - Training outputs (mounted from host)
#   /app/configs/       - Configuration files (mounted from host)
#   /app/figures/       - Generated plots
#   /app/checkpoints/   - Model checkpoints

RUN mkdir -p /app/data/.cache \
    && mkdir -p /app/runs \
    && mkdir -p /app/figures \
    && mkdir -p /app/configs \
    && mkdir -p /app/checkpoints

# Change ownership of the entire /app directory to boenet user
# This must happen BEFORE the COPY command and BEFORE switching users
# CRITICAL: This ensures boenet can write to /app/data/.cache even when
# /app/data is mounted from the host (the .cache subdirectory permissions
# are determined by the container, not the host mount)
RUN chown -R boenet:boenet /app

# Now switch to non-root user
USER boenet

# Copy application code (as boenet user, directories already exist with correct ownership)
COPY --chown=boenet:boenet . .

# ----------------------------------------------------------------------------
# Environment Configuration
# ----------------------------------------------------------------------------
# CUDA settings
ENV NVIDIA_VISIBLE_DEVICES=all \
    NVIDIA_DRIVER_CAPABILITIES=compute,utility

# BoeNet settings (language model)
# Updated for v4.0.0 BPE tokenization defaults
ENV BOENET_DEVICE=auto \
    BOENET_DATA_ROOT=/app/data \
    BOENET_RUNS_ROOT=/app/runs \
    BOENET_CONFIG=/app/configs/experiment-config.yaml \
    BOENET_LOG_LEVEL=INFO \
    BOENET_DATASET=wikitext2 \
    BOENET_TOKENIZER_TYPE=bpe \
    BOENET_BPE_ENCODING=cl100k_base \
    BOENET_VOCAB_SIZE=100277 \
    BOENET_SEQ_LEN=128 \
    BOENET_EMBED_DIM=256 \
    BOENET_HIDDEN_DIM=512

# ----------------------------------------------------------------------------
# HuggingFace Cache Configuration (CRITICAL FIX v1.0.3)
# ----------------------------------------------------------------------------
# PROBLEM (v1.0.2 and earlier):
#   When HF_HOME was set to /home/boenet/.cache/huggingface, HuggingFace could
#   download datasets, but when running boenet_training_matrix.py which spawns
#   subprocesses, the subprocess environment sometimes didn't inherit the
#   runtime -e HF_HOME override, causing permission errors.
#
# SOLUTION (v1.0.3):
#   Set HF_HOME and HF_DATASETS_CACHE to /app/data/.cache in the Dockerfile.
#   This directory is:
#     1. Created during build with correct boenet ownership
#     2. Inside the mounted /app/data volume, so it persists
#     3. Automatically inherited by all subprocesses
#     4. Works on both Linux and Windows Docker hosts
#
# BENEFITS:
#   - No need to pass -e HF_HOME at runtime
#   - Cache persists across container runs (no re-downloading)
#   - Subprocesses inherit the correct cache location
#   - Works with volume mounts on Windows (NTFS) and Linux (ext4)
#
# NOTE: If the host /app/data directory doesn't exist before mounting,
# Docker will create it. The .cache subdirectory will be created by
# HuggingFace on first download.

ENV HF_DATASETS_CACHE=/app/data/.cache \
    HF_HOME=/app/data/.cache

# ----------------------------------------------------------------------------
# tiktoken Cache Configuration (v1.1.0)
# ----------------------------------------------------------------------------
# tiktoken downloads encoding files on first use.
# Set TIKTOKEN_CACHE_DIR to persist these across container runs.
ENV TIKTOKEN_CACHE_DIR=/app/data/.cache/tiktoken

# PyTorch CUDA settings
# CRITICAL: Blackwell (RTX 50 series) uses sm_120, NOT sm_100
# This was a common misconception - the correct compute capability is 12.0
# 
# Supported architectures:
#   - 6.0, 6.1: Pascal (GTX 10 series)
#   - 7.0: Volta (Titan V)
#   - 7.5: Turing (RTX 20 series)
#   - 8.0: Ampere (A100, RTX 30 series)
#   - 8.6: Ampere (RTX 30 series consumer)
#   - 8.9: Ada Lovelace (RTX 40 series)
#   - 9.0: Hopper (H100)
#   - 12.0: Blackwell (RTX 50 series) - sm_120
ENV TORCH_CUDA_ARCH_LIST="6.0 6.1 7.0 7.5 8.0 8.6 8.9 9.0 12.0" \
    PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True

# Performance settings
ENV OMP_NUM_THREADS=4 \
    MKL_NUM_THREADS=4

# ----------------------------------------------------------------------------
# Health Check
# ----------------------------------------------------------------------------
HEALTHCHECK --interval=30s --timeout=10s --start-period=10s --retries=3 \
    CMD python -c "import torch; assert torch.cuda.is_available(), 'CUDA not available'; from boenet.model import BoeNet; import datasets; import tiktoken; print('OK')" || exit 1

# ----------------------------------------------------------------------------
# Default Command
# ----------------------------------------------------------------------------
CMD ["python", "train_boenet.py", "--help"]

# ----------------------------------------------------------------------------
# Labels
# ----------------------------------------------------------------------------
LABEL maintainer="BoeNet Project" \
    version="1.1.0" \
    description="BoeNet CUDA Training and Inference Container for Language Models with BPE Tokenization and Blackwell Support" \
    model.type="language" \
    model.architecture="BFS+REINFORCE" \
    model.task="next-token-prediction" \
    tokenizer.default="bpe" \
    tokenizer.bpe.encoding="cl100k_base" \
    tokenizer.bpe.vocab_size="100277" \
    tokenizer.char.vocab_size="256" \
    cuda.version="12.8" \
    cudnn.version="bundled" \
    pytorch.version="2.7.1" \
    pytorch.cuda.index="cu128" \
    blackwell.compute.capability="sm_120" \
    supported.architectures="Pascal, Volta, Turing, Ampere, Ada Lovelace, Hopper, Blackwell" \
    supported.datasets="wikitext2, wikitext103, shakespeare, tinystories, bookcorpus, openwebtext, textfile" \
    default.dataset="wikitext2" \
    features="BPE tokenization, tqdm progress bars, YAML config, text generation, perplexity metrics, policy analysis, HuggingFace datasets" \
    scripts.train="train_boenet.py" \
    scripts.infer="infer_boenet.py" \
    scripts.matrix="boenet_training_matrix.py" \
    scripts.test="test_true_sparsity_boenet.py" \
    org.opencontainers.image.source="https://github.com/boenet/boenet" \
    org.opencontainers.image.title="BoeNet CUDA" \
    org.opencontainers.image.description="GPU-accelerated container for BoeNet language model training and inference with BPE tokenization (cl100k_base) and RTX 50 series (Blackwell sm_120) support"

# ============================================================================
# Usage Examples
# ============================================================================
#
# 1. TRAINING WITH BPE TOKENIZATION (v4.0.0 - RECOMMENDED):
#    docker run --rm --gpus all \
#        -v $(pwd)/data:/app/data \
#        -v $(pwd)/runs:/app/runs \
#        boenet:cuda python train_boenet.py \
#            --tokenizer_type bpe \
#            --bpe_encoding cl100k_base \
#            --embed_dim 256 \
#            --hidden_dim 512 \
#            --max_depth 4 \
#            --epochs 10
#
# 2. TRAINING ON WIKITEXT-2 (Character-level - Legacy):
#    docker run --rm --gpus all \
#        -v $(pwd)/data:/app/data \
#        -v $(pwd)/runs:/app/runs \
#        boenet:cuda python train_boenet.py \
#            --tokenizer_type char \
#            --embed_dim 64 \
#            --hidden_dim 128 \
#            --dataset wikitext2 \
#            --epochs 10 \
#            --seq_len 128 \
#            --max_depth 2 \
#            --max_children 3 \
#            --lambda_efficiency 0.05 \
#            --greedy_threshold 0.42
#
# 3. TRAINING ON SHAKESPEARE:
#    docker run --rm --gpus all \
#        -v $(pwd)/data:/app/data \
#        -v $(pwd)/runs:/app/runs \
#        boenet:cuda python train_boenet.py \
#            --dataset shakespeare \
#            --epochs 10 \
#            --seq_len 128
#
# 4. TRAINING ON TINYSTORIES (Larger Dataset):
#    docker run --rm --gpus all \
#        -v $(pwd)/data:/app/data \
#        -v $(pwd)/runs:/app/runs \
#        boenet:cuda python train_boenet.py \
#            --dataset tinystories \
#            --tokenizer_type bpe \
#            --epochs 20 \
#            --seq_len 256 \
#            --embed_dim 256 \
#            --hidden_dim 512 \
#            --batch_size 16
#
# 5. TEXT GENERATION:
#    docker run --rm -it --gpus all \
#        -v $(pwd)/runs:/app/runs \
#        boenet:cuda python infer_boenet.py \
#            --ckpt runs/checkpoint.pt \
#            --generate \
#            --prompt "To be or not to be" \
#            --max_tokens 200 \
#            --temperature 0.8 \
#            --top_k 40
#
# 6. POLICY ANALYSIS (Debug):
#    docker run --rm --gpus all \
#        -v $(pwd)/runs:/app/runs \
#        boenet:cuda python infer_boenet.py \
#            --ckpt runs/checkpoint.pt \
#            --debug_policy \
#            --samples 1000
#
# 7. FULL SWEEP WITH BPE (Background):
#    docker run -d --gpus all \
#        --name boenet_bpe_sweep \
#        -v $(pwd)/data:/app/data \
#        -v $(pwd)/runs:/app/runs \
#        -v $(pwd)/configs:/app/configs \
#        -v $(pwd)/boenet:/app/boenet \
#        boenet:cuda python boenet_training_matrix.py \
#            --config configs/experiment-config.yaml
#
#    # Monitor:
#    docker logs -f boenet_bpe_sweep
#
# 8. QUICK TEST (CI/CD):
#    docker run --rm --gpus all \
#        -v $(pwd)/data:/app/data \
#        -v $(pwd)/runs:/app/runs \
#        -v $(pwd)/configs:/app/configs \
#        boenet:cuda python boenet_training_matrix.py \
#            --config configs/test-config.yaml
#
# 9. SPARSITY TESTS:
#    docker run --rm --gpus all \
#        boenet:cuda python test_true_sparsity_boenet.py
#
# 10. INTERACTIVE SHELL:
#     docker run --rm -it --gpus all \
#         -v $(pwd):/app \
#         boenet:cuda bash
#
# 11. WINDOWS POWERSHELL (use ${PWD} instead of $(pwd)):
#     docker run --rm `
#         -v ${PWD}/data:/app/data `
#         -v ${PWD}/runs:/app/runs `
#         -v ${PWD}/configs:/app/configs `
#         -v ${PWD}/boenet:/app/boenet `
#         boenet:cuda python boenet_training_matrix.py --config configs/experiment-config.yaml
#
# ============================================================================

# ============================================================================
# Environment Variables Reference
# ============================================================================
#
# BOENET_DEVICE:         Device selection (auto, cuda, cpu)
# BOENET_DATA_ROOT:      Data directory path
# BOENET_RUNS_ROOT:      Output/runs directory path
# BOENET_CONFIG:         Default config file path
# BOENET_LOG_LEVEL:      Logging level (DEBUG, INFO, WARNING, ERROR)
# BOENET_DATASET:        Default dataset (wikitext2, shakespeare, tinystories)
# BOENET_TOKENIZER_TYPE: Tokenizer type (bpe, char) - NEW v1.1.0
# BOENET_BPE_ENCODING:   BPE encoding (cl100k_base, gpt2) - NEW v1.1.0
# BOENET_VOCAB_SIZE:     Vocabulary size (100277 for BPE, 256 for char)
# BOENET_SEQ_LEN:        Default sequence length
# BOENET_EMBED_DIM:      Default embedding dimension (256 for BPE, 64 for char)
# BOENET_HIDDEN_DIM:     Default hidden dimension (512 for BPE, 128 for char)
# HF_DATASETS_CACHE:     HuggingFace datasets cache directory (/app/data/.cache)
# HF_HOME:               HuggingFace home directory (/app/data/.cache)
# TIKTOKEN_CACHE_DIR:    tiktoken encoding cache directory (/app/data/.cache/tiktoken)
#
# Override any of these at runtime:
#   docker run --rm --gpus all \
#       -e BOENET_DATASET=tinystories \
#       -e BOENET_TOKENIZER_TYPE=bpe \
#       -e BOENET_EMBED_DIM=256 \
#       boenet:cuda python train_boenet.py
#
# ============================================================================

# ============================================================================
# Volume Mounts Reference
# ============================================================================
#
# /app/data:        Dataset storage AND HuggingFace/tiktoken cache (/app/data/.cache)
# /app/runs:        Training outputs (checkpoints, logs, CSVs)
# /app/configs:     YAML configuration files
# /app/figures:     Generated plots and visualizations
# /app/checkpoints: Model checkpoints (alternative to runs/)
# /app/boenet:      (Optional) Mount source code for development
#
# Recommended mount command (Linux/macOS):
#   docker run --rm --gpus all \
#       -v $(pwd)/data:/app/data \
#       -v $(pwd)/runs:/app/runs \
#       -v $(pwd)/configs:/app/configs \
#       -v $(pwd)/boenet:/app/boenet \
#       boenet:cuda <command>
#
# Recommended mount command (Windows PowerShell):
#   docker run --rm `
#       -v ${PWD}/data:/app/data `
#       -v ${PWD}/runs:/app/runs `
#       -v ${PWD}/configs:/app/configs `
#       -v ${PWD}/boenet:/app/boenet `
#       boenet:cuda <command>
#
# NOTE (v1.0.3): HuggingFace cache is now stored in /app/data/.cache by default.
# NOTE (v1.1.0): tiktoken cache is now stored in /app/data/.cache/tiktoken.
# This means datasets and tokenizer encodings are automatically persisted.
# No separate cache mount is needed!
#
# ============================================================================

# ============================================================================
# Troubleshooting
# ============================================================================
#
# 1. "CUDA not available" error:
#    - Ensure NVIDIA Container Toolkit is installed
#    - Run: nvidia-smi (should show GPU info)
#    - Use --gpus all flag
#    - On Windows: Enable WSL2 GPU support in Docker Desktop settings
#
# 2. "Out of memory" error:
#    - Reduce batch_size (e.g., 16 or 8 for BPE models)
#    - Reduce seq_len (e.g., 64)
#    - Reduce hidden_dim (e.g., 256 instead of 512)
#    - BPE models are ~78M params vs ~150K for char - need more memory!
#
# 3. "Dataset not found" or "datasets library not available" error:
#    - The datasets library should be pre-installed in this container
#    - If running locally: pip install datasets
#    - Ensure data volume is mounted: -v $(pwd)/data:/app/data
#    - Dataset downloads automatically on first run
#
# 4. "tiktoken is not available" error:
#    - FIXED in v1.1.0: tiktoken is now pre-installed
#    - If running locally: pip install tiktoken
#    - Verify: python -c "import tiktoken; print(tiktoken.__version__)"
#    - If still failing, rebuild the Docker image
#
# 5. "Permission denied" error for HuggingFace/tiktoken cache:
#    - FIXED in v1.0.3/v1.1.0: Caches now use /app/data/.cache
#    - This directory is inside the mounted volume and writable
#    - Make sure /app/data is mounted: -v $(pwd)/data:/app/data
#    - On Windows, ensure Docker has access to the drive
#
# 6. "Permission denied" error for other directories:
#    - Check volume ownership
#    - Run: docker run --rm -v $(pwd)/data:/app/data boenet:cuda ls -la /app/data
#    - Ensure host directories exist before mounting
#    - On Windows: Check Docker Desktop -> Settings -> Resources -> File Sharing
#
# 7. Slow training:
#    - Verify GPU is being used: watch nvidia-smi
#    - Check batch_size (larger = faster but more memory)
#    - Use TensorFloat-32: export NVIDIA_TF32_OVERRIDE=1
#    - BPE models take longer per epoch than char models (more params)
#
# 8. Text generation produces garbage:
#    - Model may not be trained enough (increase epochs)
#    - Try lower temperature (0.5-0.8)
#    - Use top_k or top_p sampling
#    - BPE models should produce much better text than char models
#
# 9. HuggingFace/tiktoken download issues:
#    - Check internet connectivity
#    - Try setting HF_DATASETS_OFFLINE=1 if datasets are already cached
#    - Cache is stored in /app/data/.cache (persists with volume mount)
#    - tiktoken downloads encoding files on first use to /app/data/.cache/tiktoken
#
# 10. "trust_remote_code is not supported anymore" warning:
#     - FIXED in data_utils.py v2.0.0: Removed deprecated parameter
#     - This warning is harmless and can be ignored in older versions
#
# 11. Windows-specific issues:
#     - Use ${PWD} instead of $(pwd) in PowerShell
#     - Use %cd% in CMD
#     - Ensure Docker Desktop is running
#     - Enable WSL2 backend for best performance
#
# 12. BPE tokenizer issues:
#     - Verify tiktoken is installed: python -c "import tiktoken"
#     - Verify encoding: python -c "import tiktoken; tiktoken.get_encoding('cl100k_base')"
#     - If using --tokenizer_type char, tiktoken is not needed
#
# ============================================================================