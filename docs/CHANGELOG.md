# Changelog

All notable changes to the BFSNet ‚Üí BoeNet project are documented in this file.

The format is based on [Keep a Changelog](https://keepachangelog.com/en/1.0.0/),
and this project adheres to [Semantic Versioning](https://semver.org/spec/v2.0.0.html).

**‚ö†Ô∏è Proprietary Software**: This project is closed source. All rights reserved.

---

## Project Evolution
```
BFSNet (Vision)          ‚Üí          BoeNet (Language)
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ                    ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
FashionMNIST                        Character-level ‚Üí Word-level ‚Üí Production
REINFORCE on images                 REINFORCE on sequences
v1.0.0 - v2.0.0 FINAL              v0.1.0 - IN PROGRESS
‚úÖ COMPLETE                         üöß ACTIVE DEVELOPMENT
```

---

## [BoeNet v0.1.0] - IN PROGRESS (January 2026)

**üöß PROJECT INITIATED - ACTIVE DEVELOPMENT**

This version marks the transition from BFSNet (vision) to BoeNet (language modeling). BoeNet applies BFS tree expansion with REINFORCE policy gradients to sequential text processing, starting with character-level proof of concept.

### Added

#### Core Architecture
- üöß **BFSLanguageCell**: Recurrent BFS cell for processing sequences token-by-token
- üöß **BoeNet Model**: Stacked BFSLanguageCell layers with hidden state propagation
- üöß **Character-level Tokenization**: ASCII tokenizer for Phase 1 (Shakespeare)
- ‚è≥ **BPE Tokenization**: Byte-pair encoding for Phase 2 (TinyStories)
- üöß **Text Generation**: Autoregressive sampling with temperature/top-k/top-p
- üöß **Perplexity Metrics**: Language model evaluation

#### Dataset Support
- ‚úÖ **Shakespeare Download Script**: `scripts/boenet/download_shakespeare.py` (~1MB)
- ‚úÖ **TinyStories Download Script**: `scripts/boenet/download_tinystories.py` (~2GB)
- üöß **Text Preprocessing Pipeline**: `scripts/boenet/preprocess_text.py`
- üöß **Dataset Loaders**: Character-level and word-level data loaders

#### Docker Infrastructure
- üöß **BoeNet CPU Docker**: `docker/Dockerfile.boenet` (Python 3.11, PyTorch 2.7.1 CPU)
- üöß **BoeNet CUDA Docker**: `docker/Dockerfile.boenet.cuda` (CUDA 12.8, Blackwell support)
- ‚úÖ **Language Dependencies**: tokenizers, transformers, datasets, sentencepiece
- ‚úÖ **Text Data Volumes**: `/app/data/text`, `/app/data/tokenizers`, `/app/data/processed`

#### Configuration Files
- üöß **configs/boenet/char-level-test.yaml**: Phase 1 Shakespeare minimal config
- üöß **configs/boenet/char-level-full.yaml**: Phase 1 War and Peace full config
- ‚è≥ **configs/boenet/word-level-tiny.yaml**: Phase 2 TinyStories config
- ‚úÖ **configs/boenet/README.md**: BoeNet configuration guide

#### Testing Infrastructure
- üöß **tests/boenet/unit/test_tokenization.py**: Character and BPE tokenization tests
- üöß **tests/boenet/unit/test_bfs_language_cell.py**: BFSLanguageCell unit tests
- üöß **tests/boenet/unit/test_sequence_processing.py**: Sequence batching tests
- ‚è≥ **tests/boenet/unit/test_generation.py**: Text generation tests
- ‚è≥ **tests/boenet/unit/test_perplexity.py**: Perplexity calculation tests
- ‚è≥ **tests/boenet/integration/test_char_training.py**: E2E character-level training
- ‚è≥ **tests/boenet/TEST_PLAN.md**: Phase 1 test strategy

#### Scripts & Utilities
- ‚úÖ **scripts/boenet/download_shakespeare.py**: READY
- ‚úÖ **scripts/boenet/download_tinystories.py**: READY
- üöß **scripts/boenet/preprocess_text.py**: Text preprocessing utilities
- üöß **scripts/boenet/tokenizer_utils.py**: Tokenizer training and testing
- üöß **scripts/boenet/generate_text.py**: Text generation from checkpoints
- ‚è≥ **scripts/boenet/analyze_perplexity.py**: Perplexity analysis and comparison

#### Documentation
- ‚úÖ **README.md**: Updated with BoeNet vision, roadmap, and architecture
- üöß **docs/boenet_architecture.md**: BoeNet technical specification (IN PROGRESS)
- ‚úÖ **configs/README.md**: Updated with BoeNet configuration examples
- ‚úÖ **docker/README.md**: Updated with BoeNet Docker setup
- ‚úÖ **scripts/README.md**: Updated with BoeNet script documentation
- ‚úÖ **tests/README.md**: Updated with BoeNet test strategy
- ‚úÖ **BFSNet Docker & Testing Architecture Specification.md**: Added Part II (BoeNet)

### Changed

#### Architecture Evolution
- **Input Processing**: Images (784-dim) ‚Üí Sequences (variable-length tokens)
- **Output Format**: Class logits (10-dim) ‚Üí Token logits per position (vocab_size √ó seq_len)
- **Metric**: Accuracy (%) ‚Üí Perplexity
- **Reward Function**: `acc - Œª √ó nodes` ‚Üí `-perplexity - Œª √ó FLOPs`
- **Processing Model**: Single-shot feedforward ‚Üí Recurrent BFS per token

#### Configuration Schema
- **Added**: `vocab_size`, `embed_dim`, `seq_len`, `num_layers` (language-specific)
- **Added**: `max_new_tokens`, `temperature`, `top_k`, `top_p` (generation)
- **Retained**: `max_children`, `max_depth`, `lambda_efficiency`, `greedy_threshold` (from BFSNet)
- **Retained**: `num_rollouts`, `beta_entropy` (REINFORCE params)

#### Docker Environment
- **Base Image**: Python 3.10 ‚Üí Python 3.11 (latest stable)
- **PyTorch**: 2.1.0 (BFSNet CPU) ‚Üí 2.7.1 (BoeNet CPU/CUDA)
- **Dependencies**: Added tokenizers, transformers, datasets, sentencepiece
- **Volumes**: Images (`/data/FashionMNIST`) ‚Üí Text files (`/data/text`)

### Lessons Applied from BFSNet

The following insights from BFSNet v2.0.0 informed BoeNet design:

1. **REINFORCE Reliability**: 
   - BFSNet: Policy gradients converged stably (98% test pass rate)
   - BoeNet: Using same REINFORCE approach with confidence

2. **Efficiency as Regularization**:
   - BFSNet: Œª=0.05 achieved better accuracy than Œª=0.01 (87.42% vs 86.62%)
   - BoeNet: Starting with Œª=0.05, treating as regularization parameter

3. **Threshold Mismatch Critical**:
   - BFSNet: Default threshold 0.5 caused zero children (policy learned ~0.44)
   - BoeNet: Implementing `--debug_policy` from day 1, planning adaptive thresholds

4. **Policy Learns Tight Distributions**:
   - BFSNet: 98% of grow_prob in [0.4, 0.5), std=0.0157
   - BoeNet: Expecting similar, planning threshold tuning from start

5. **Batch Normalization Bug**:
   - BFSNet: Batch norm in rewards caused batch-dependent rewards (FIXED)
   - BoeNet: Ensuring sample-independent reward calculation

6. **Root-Only Baseline Strong**:
   - BFSNet: Root-only achieved 86-87% (vs 87.42% full tree)
   - BoeNet: Validating that depth is needed for language (may differ from vision)

7. **Statistical Validation Important**:
   - BFSNet: Single-seed results (repeats=1) - should have used 3+
   - BoeNet: Planning 3+ seeds for Phase 1 validation

8. **Latency Percentiles Matter**:
   - BFSNet: p99 was 42√ó higher than p50 (outliers!)
   - BoeNet: Tracking p50, p90, p99 from start

### Development Roadmap

**Phase 1: Character-Level (Weeks 1-6) - CURRENT**
- üöß Week 1-2: BFSLanguageCell implementation, tokenization
- üöß Week 3-4: Training pipeline, perplexity tracking
- ‚è≥ Week 5-6: Text generation, baseline comparison (LSTM)

**Phase 2: Word-Level (Weeks 7-12) - PLANNED**
- ‚è≥ BPE tokenization (GPT-2 vocab)
- ‚è≥ TinyStories dataset (2M stories, 2GB)
- ‚è≥ 25M parameter model
- ‚è≥ Coherent 2-3 sentence generation

**Phase 3: Production Scale (Months 4-6) - PLANNED**
- ‚è≥ 125M-1B parameters
- ‚è≥ OpenWebText ‚Üí The Pile datasets
- ‚è≥ Standard LLM benchmarks (MMLU, HellaSwag)

**Phase 4: Arcus LLM (Months 7-12+) - PLANNED**
- ‚è≥ 7B+ parameters
- ‚è≥ ChatGPT-level performance goal
- ‚è≥ Personal language model

### Success Criteria (Phase 1)

**Minimum Success**:
- [ ] Character-level perplexity ‚â§ LSTM baseline
- [ ] 30-50% FLOPs reduction vs full tree expansion
- [ ] Coherent character-by-character generation

**Target Success**:
- [ ] Perplexity matches LSTM within 5%
- [ ] Policy converges stably (no NaN, no collapse)
- [ ] Adaptive threshold tuning works

**Stretch Success**:
- [ ] Perplexity beats LSTM by 5%+
- [ ] 50%+ FLOPs reduction
- [ ] Ready to scale to Phase 2 (word-level)

### Known Issues

- ‚ö†Ô∏è Text dataset mounting is manual (no auto-download like FashionMNIST)
- ‚ö†Ô∏è Threshold tuning methodology adapted from vision, may need adjustment
- ‚ö†Ô∏è No baseline implementations yet (LSTM, Transformer for comparison)

### References

- See `docs/boenet_architecture.md` for technical specification (IN PROGRESS)
- See `BOENET_VISION.md` for project goals and motivation
- See `docs/bfsnet_architecture.md` for lessons from vision phase

---

## [BFSNet v2.0.0] - 2025-12-18 **[FINAL RELEASE]**

**‚úÖ PROJECT COMPLETE - NO FURTHER DEVELOPMENT**

This is the **final release** of BFSNet (vision). All development on FashionMNIST experiments is complete. The project successfully demonstrated that BFS tree expansion with REINFORCE policy gradients works for neural networks.

### Summary

BFSNet v2.0.0 achieved **87.42% validation accuracy** on FashionMNIST, beating the dense baseline (85%) by 2.42 percentage points. This validates the core concept of adaptive tree expansion with policy gradients and provides the foundation for BoeNet (language modeling).

### Added

#### Final Features
- ‚úÖ **Complete 48-configuration parameter sweep** on FashionMNIST
- ‚úÖ **Greedy threshold tuning capability** via `--debug_policy` flag
- ‚úÖ **Policy distribution analysis** in inference script
- ‚úÖ **Comprehensive logging** with JSON summary format (`__SUMMARY__` tags)

#### Documentation (COMPLETE)
- ‚úÖ **docs/bfsnet_architecture.md**: Complete technical retrospective
- ‚úÖ **Bfsnet fashionmnist test plan.md**: Final results and lessons learned
- ‚úÖ **tests/bfsnet/RESULTS.md**: Complete test suite results
- ‚úÖ **BFSNET_FINAL_REPORT.md**: Executive summary
- ‚úÖ **BFSNet Docker & Testing Architecture Specification.md**: Part I (Vision)

#### Test Suite (COMPLETE)
- ‚úÖ **57+ tests** total (45 unit, 12 integration)
- ‚úÖ **98% pass rate** (56/57 passed)
- ‚úÖ **86% code coverage** on core functionality
- ‚úÖ All tests documented in `tests/bfsnet/RESULTS.md`

### Changed

#### Critical Fixes
- ‚úÖ **FIXED: Batch Normalization Bug** in efficiency penalty calculation
  - Issue: Batch norm in `_compute_rewards()` made rewards batch-dependent
  - Impact: Incorrect efficiency penalty, rewards not sample-independent
  - Fix: Moved batch norm outside reward calculation
  - Status: VERIFIED in unit tests

- ‚úÖ **IMPROVED: Inference JSON Parsing**
  - Added `__SUMMARY__` JSON tag parsing
  - Robust handling of malformed JSON
  - Validation of metric ranges (accuracy 0-100%)

- ‚úÖ **IMPROVED: Training Matrix CSV Output**
  - All metrics properly captured in CSV
  - Run IDs unique across experiments
  - No NaN in core metrics
  - JSONL format added for easier parsing

### Discovered Issues & Insights

#### 1. Greedy Threshold Mismatch (CRITICAL FINDING)

**Issue**: Default `greedy_threshold=0.5` caused ZERO children to be created in inference.

**Root Cause**:
- Training: Stochastic Bernoulli(grow_prob) sampling
- Inference: Deterministic `grow_prob >= threshold` decision
- Policy learned: grow_prob ‚âà 0.44-0.45 (below threshold!)

**Evidence**:
```
Policy Distribution (Œª=0.01, 1200 decisions):
  Mean:    0.4457
  Std dev: 0.0157
  Min:     0.3771
  Max:     0.4567
  % ‚â• 0.5: 0.00%  ‚Üê ZERO decisions above threshold!
```

**Impact**:
- Inference with threshold=0.5 ‚Üí root-only (1 node) ‚Üí 86.95% accuracy
- Inference with threshold=0.42 ‚Üí partial tree (~8 nodes) ‚Üí ~88% accuracy (estimated)

**Workaround**: Set `greedy_threshold ‚âà mean_grow_prob - 0.03` (empirically ~0.42)

**Status**: DOCUMENTED, workaround implemented

---

#### 2. Higher Lambda ‚Üí Better Accuracy (COUNTER-INTUITIVE)

**Finding**: Stronger efficiency penalty improved accuracy!

**Evidence**:
| Œª | Training Nodes | Val Accuracy | Analysis |
|---|----------------|--------------|----------|
| 0.05 | 6.44 | **87.42%** | ‚úÖ Best accuracy |
| 0.01 | 11.80 | **86.62%** | Worse with MORE nodes |

**Hypothesis**:
- Higher Œª acts as **regularization** (forces selectivity)
- Fewer nodes ‚Üí faster forward pass ‚Üí more gradient steps per epoch
- Task-specific: FashionMNIST may not need deep trees

**Implication**: Efficiency penalty is not just a speed knob, it affects quality!

**Status**: VALIDATED across multiple configurations

---

#### 3. Root-Only Performance Strong

**Finding**: Root-only (1 node) achieved 86-87% accuracy.

**Evidence**:
- Root-only (threshold=0.5): 86.95%
- Full tree (threshold=0.3): 86.95% (no improvement!)
- Partial tree (threshold=0.42): ~88% (marginal gain)

**Interpretation**: FashionMNIST may not require hierarchical reasoning.

**Implication**: Task may be too simple for BFS to shine; language modeling likely better fit.

**Status**: DOCUMENTED

---

#### 4. Policy Learns Narrow Distributions

**Finding**: grow_prob converged to tight range regardless of Œª.

**Evidence**:
- Œª=0.05: mean=0.4457, std=0.0157
- Œª=0.01: mean=0.4450, std=0.0160
- 98% of decisions in [0.4, 0.5)

**Interpretation**: Policy optimized for training dynamics, not final threshold.

**Status**: EXPECTED BEHAVIOR

---

### Fixed

- ‚úÖ Batch normalization in reward calculation (sample independence)
- ‚úÖ Inference JSON parsing robustness
- ‚úÖ CSV output formatting and validation
- ‚úÖ Device fallback warnings (CUDA/MPS ‚Üí CPU)
- ‚úÖ Gradient flow verification (all layers receive gradients)
- ‚úÖ Checkpoint save/load round-trip

### Performance Metrics (Best Configuration)

**Configuration**: Œª=0.05, K=3, depth=2, threshold=0.42

| Metric | Value |
|--------|-------|
| **Validation Accuracy** | **87.42%** |
| **Test Accuracy (threshold=0.5)** | **86.95%** |
| **Test Accuracy (threshold=0.42, est.)** | **~88%** |
| **Training Nodes/Example** | **6.44** |
| **Inference Nodes (threshold=0.5)** | **1.0** (root-only) |
| **Inference Nodes (threshold=0.42)** | **~8** (partial tree) |
| **Inference Latency (p50)** | **0.60 ms** |
| **Inference Latency (p99)** | **25.47 ms** (outliers!) |
| **Dense Baseline** | **~85%** |
| **Improvement over Dense** | **+2.42%** |

### Test Results Summary

**Unit Tests**: 45 tests, 44 passed, 1 known difference
- ‚úÖ Gradient flow verified
- ‚úÖ Dense baseline (K=0) matches MLP
- ‚ö†Ô∏è Sparse/dense gradient magnitudes differ (expected)
- ‚úÖ Checkpoint round-trip works
- ‚úÖ Device fallback robust
- ‚úÖ Edge cases handled
- ‚úÖ Numerically stable
- ‚úÖ Execution modes work

**Integration Tests**: 12 tests, 12 passed
- ‚úÖ Pipeline smoke test
- ‚úÖ CSV output validation
- ‚úÖ Config loading

**Coverage**: 86% (835 statements, 113 missed)

**Pass Rate**: 98.2% (56/57)

### Known Issues (Unfixed)

1. ‚ö†Ô∏è **Greedy threshold must be manually tuned**
   - Not learned automatically
   - Requires post-training measurement via `--debug_policy`
   - **Workaround**: Set threshold ‚âà mean_grow_prob - 0.03
   - **Impact**: Medium - workaround documented

2. ‚ö†Ô∏è **Training/inference mode mismatch**
   - Stochastic sampling in training vs deterministic threshold in inference
   - Causes unexpected behavior if threshold not tuned
   - **Workaround**: Threshold tuning
   - **Impact**: Medium - can be mitigated

3. ‚ö†Ô∏è **Latency p99 outliers**
   - 99th percentile 42√ó higher than median (25ms vs 0.6ms)
   - Likely JIT compilation, CPU scheduling, memory spikes
   - **Impact**: Low for most use cases, problematic for strict SLAs

4. ‚ö†Ô∏è **FashionMNIST may not require BFS**
   - Root-only achieves 86-87% (full tree only adds ~1%)
   - Task may be too simple to benefit from adaptive compute
   - **Impact**: Low - validates architecture, may not generalize to harder tasks

### Deprecated

- ‚ö†Ô∏è v1.x dense-then-mask approach (replaced by true sparse execution)
- ‚ö†Ô∏è Old JSON parsing (replaced by `__SUMMARY__` tag parsing)
- ‚ö†Ô∏è Implicit threshold=0.5 assumption (now explicit and tunable)

### Migration Guide (v1.4.0 ‚Üí v2.0.0)

**Not Applicable**: BFSNet development is COMPLETE. No migration needed.

For those referencing v1.x code:
1. Replace dense-then-mask with true sparse execution (v2.0.0 model)
2. Add explicit `greedy_threshold` tuning
3. Use `--debug_policy` to measure policy distribution
4. Update configs to include `lambda_efficiency` explicitly

### References

- **Architecture**: `docs/bfsnet_architecture.md`
- **Test Results**: `tests/bfsnet/RESULTS.md`
- **Test Plan**: `Bfsnet fashionmnist test plan.md`
- **Docker Setup**: `docker/README.md`
- **Configuration**: `configs/bfsnet/README.md`

---

## [BFSNet v1.4.0] - 2025-12-11 **[LEGACY]**

**‚ö†Ô∏è SUPERSEDED BY v2.0.0 - Historical reference only**

### Added
- Initial BFS tree expansion implementation
- Dense-then-mask approach (not true sparse)
- FashionMNIST training pipeline
- Basic Docker support

### Known Issues (Fixed in v2.0.0)
- ‚ùå Dense-then-mask wastes computation
- ‚ùå No policy distribution analysis
- ‚ùå Batch norm bug in rewards
- ‚ùå Threshold hardcoded to 0.5

---

## [BFSNet v1.0.0] - 2025-12-01 **[LEGACY]**

**‚ö†Ô∏è SUPERSEDED BY v2.0.0 - Historical reference only**

### Added
- Initial project setup
- Basic BFS model architecture
- FashionMNIST data loading
- Simple MLP baseline

---

## Legend

### Status Indicators
- ‚úÖ **COMPLETE**: Feature/task is finished and validated
- üöß **IN PROGRESS**: Feature/task is being actively developed
- ‚è≥ **PLANNED**: Feature/task is planned but not started
- ‚ö†Ô∏è **PARTIAL**: Feature/task is partially complete
- ‚ùå **DEPRECATED**: Feature/task is no longer supported

### Priority Indicators
- üî¥ **CRITICAL**: Must be completed for project to function
- üü° **HIGH**: Important for project success
- üü¢ **MEDIUM**: Nice to have, improves project
- ‚ö™ **LOW**: Optional, future consideration

---

## Version Numbering

This project uses [Semantic Versioning](https://semver.org/):

**Format**: MAJOR.MINOR.PATCH

- **MAJOR**: Incompatible architecture changes (BFSNet ‚Üí BoeNet = major)
- **MINOR**: Backwards-compatible functionality additions
- **PATCH**: Backwards-compatible bug fixes

**BFSNet Versions**:
- v1.x.x: Initial development (LEGACY)
- v2.0.0: Final release (COMPLETE)

**BoeNet Versions**:
- v0.1.0: Phase 1 character-level (IN PROGRESS)
- v0.2.0: Phase 2 word-level (PLANNED)
- v0.3.0: Phase 3 production scale (PLANNED)
- v1.0.0: Arcus LLM v1.0 (PLANNED)

---

## Contributing

**‚ö†Ô∏è IMPORTANT**: This is a **closed-source, proprietary project**. 

Contributions are limited to:
- Authorized collaborators only
- Code review and feedback by invitation
- Bug reports (if given access)

**For collaboration inquiries, contact the project owner.**

---

## Links

- **Documentation**: `docs/` directory
- **Architecture (BFSNet)**: `docs/bfsnet_architecture.md`
- **Architecture (BoeNet)**: `docs/boenet_architecture.md` (IN PROGRESS)
- **Test Results (BFSNet)**: `tests/bfsnet/RESULTS.md`
- **Test Plan (BoeNet)**: `tests/boenet/TEST_PLAN.md` (IN PROGRESS)
- **Docker Setup**: `docker/README.md`
- **Configuration Guide**: `configs/README.md`

---

## Acknowledgments

### BFSNet Phase
- PyTorch team for the excellent deep learning framework
- REINFORCE algorithm: Williams, 1992
- FashionMNIST dataset creators
- Critical threshold mismatch discovery: December 18, 2025 debug session

### BoeNet Phase
- Andrej Karpathy's nanoGPT for inspiration and methodology
- Transformer architecture: Vaswani et al., 2017
- Character-level language modeling: Karpathy et al., 2015
- The Pile dataset: Gao et al., 2020
- TinyStories dataset: Eldan & Li, 2023

---

**Last Updated**: December 20, 2025  
**Current Version**: BFSNet v2.0.0 (COMPLETE) | BoeNet v0.1.0 (IN PROGRESS)  
**Project Status**: Transitioning from vision to language modeling  
**License**: Proprietary - All rights reserved

**‚ö†Ô∏è Proprietary Software**: This project is closed source. All rights reserved.